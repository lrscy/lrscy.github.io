{"meta":{"title":"Meow","subtitle":null,"description":null,"author":"Ruosen Li","url":"https://lrscy.github.io"},"pages":[{"title":"404","date":"2020-07-09T08:11:49.681Z","updated":"2020-07-09T08:11:49.681Z","comments":false,"path":"404/index.html","permalink":"https://lrscy.github.io/404/index.html","excerpt":"","text":"123456789101112131415404 not found 4444 4444 0000000000 4444 4444 4444 4444 00000000000000 4444 4444 4444 4444 0000 0000 4444 4444 44444444444444 0000 0000 44444444444444 4444444444444 0000 0000 4444444444444 4444 0000 0000 4444 4444 0000 0000 4444 4444 00000000000000 4444 4444 0000000000 4444 not found404"},{"title":"categories","date":"2020-07-09T08:11:49.693Z","updated":"2020-07-09T08:11:49.693Z","comments":false,"path":"categories/index.html","permalink":"https://lrscy.github.io/categories/index.html","excerpt":"","text":""},{"title":"Links","date":"2020-07-09T08:11:49.693Z","updated":"2020-07-09T08:11:49.693Z","comments":true,"path":"links/index.html","permalink":"https://lrscy.github.io/links/index.html","excerpt":"","text":"Friends Links Alynx https://sh.alynx.one/ 欢迎各位同学交换链接 :)"},{"title":"tags","date":"2020-07-09T08:11:49.693Z","updated":"2020-07-09T08:11:49.693Z","comments":false,"path":"tags/index.html","permalink":"https://lrscy.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Monte Carlo Methods","slug":"Coursera-Reinforcement-Learning-Course2-Week2-Notes","date":"2020-07-09T06:29:57.000Z","updated":"2020-07-09T08:36:04.745Z","comments":true,"path":"2020/07/09/Coursera-Reinforcement-Learning-Course2-Week2-Notes/","link":"","permalink":"https://lrscy.github.io/2020/07/09/Coursera-Reinforcement-Learning-Course2-Week2-Notes/","excerpt":"Sample-based Learning Methods Week2 Notes","text":"Sample-based Learning Methods Week2 Notes Monte Carlo MethodsMonte Carlo do NOT assume complete knowledge of the environment. It requires no prior knowledge of the environment’s dynamics. It ONLY need experience–sample sequences of states, actions, and reward from actual or simulated interaction with an environment. Although a model is required, the model need only generate sample transitions, not the complete probability distributions of all possible transitions that is required for dynamic programming (DP). Monte Carlo methods are ways of solving the reinforcement learning problem based on averaging sample returns. To ensure that well-defined returns are available, here we define Monte Carlo methods only for episodic tasks. Monte Carlo methods is incremental in an episode-by-episode sense, but not in a step-by-step (online) sense. Here we use it specifically for methods based on averaging complete returns. Whereas there we computed value functions from knowledge of the MDP, here we learn value functions from sample returns with the MDP. The value functions and corresponding policies still interact to attain optimality in essentially the same way (GPI). Monte Carlo PredictionAn obvious way to estimate the expected return from experience is simply to average the returns observed after visits to that state. As more returns are observed, the average should converge to the expected value. This idea underlies all Monte Carlo methods. By the law of large numbers the sequence of averages of these estimates converges to their expected value. Each average is itself an unbiased estimate, and the standard deviation of its error falls as , where n is the number of returns averaged. Compare between Monte Carlo and DP DP Monte Carlo Backup diagram shows all possible transitions. On Backup diagram, Monte Carlo diagram shows only those sampled on the one episode. The DP diagram includes only one-step transitions. The Monte Carlo diagram goes all the way to the end of the episode on the backup diagram. DP method use successors calculated state to calculate current state value, which do not handle each states independently. Which is Bootstrap. The estimates for each state are independent. In other words, Monte Carlo methods DO NOT bootstrap. For Monte Carlo, in particular, note that the computational expense of estimating the value of a single state is independent of the number of states. One can generate many sample episodes starting from the states of interest, averaging returns from only these states, ignoring all others. Monte Carlo Estimation of Action ValuesIf a model is not available, then it is particularly useful to estimate action values (the values of state–action pairs) rather than state values. Without a model, state values alone are not sufficient. One MUST explicitly estimate the value of each action for the values to be useful in suggesting a policy. Thus, one of our primary goals for Monte Carlo methods is to estimate . The ONLY complication is that many state–action pairs may NEVER be visited. We need to estimate the value of all the actions from each state, not just the one we currently favor. This is the general problem of maintaining exploration, as discussed in the context of the k-armed bandit problem. The first way to solve it is to try infinite times with infinite number of episodes with nonzero probability to start episodes on each state-action pair so that it is guaranteed that each state-action pair is visited and has a nonzero probability. We call this the assumption of exploring starts. The most common alternative approach is to consider only policies that are stochastic with a nonzero probability of selecting all actions in each state. Obviously, in reality, we cannot gather infinite episodes from the interaction with the environment. It will be solved later. Monte Carlo ControlHere, we retain the assumption of exploring starts. The overall idea is to apply the generalized policy iteration (GPI). We perform alternating complete steps of policy evaluation and policy improvement, beginning with an arbitrary policy and ending with the optimal policy and optimal action-value function. We assume that we do indeed observe an infinite number of episodes and the episodes are generated with exploring starts. Policy improvement is done by making the policy greedy with respect to the current value function. In this case we have an action-value function, and therefore no model is needed to construct the greedy policy. The policy will be deterministically chose with maximal action-value: The policy improvement can be proved by the policy improvement theorem: In this way Monte Carlo methods can be used to find optimal policies given only sample episodes and no other knowledge of the environment’s dynamics. Monte Carlo with Exploring StartsThe algorithm pseudo-code is below, it gives the solution of infinite number of episodes showed in the next chapter: Solutions of Two AssumptionsInfinite number of episodes solution One is to hold firm to the idea of approximating in each policy evaluation. However, it is also likely to requirefar too many episodes to be useful in practice on any but the smallest problems. Another one is similar to the idea of GPI. On each evaluation step we move the value function toward , but we do not expect to actually get close except over many steps. One extreme form of the idea is to alternatively apply policy improvement and policy evaluation. Exploring Starts Solution On-policy control methods attempt to evaluate or improve the policy that is used to make decisions. Off-policy control methods evaluate or improve a policy different from that used to generate the data. On-policy MethodsIn on-policy control methods the policy is generally soft, meaning that for all and all , but gradually shifted closer and closer to a deterministic optimal policy. The overall idea of on-policy Monte Carlo control is still that of GPI. In our on-policy method we will move it only to an policy. For any policy, , any policy with respect to is guaranteed to be better than or equal to . The complete algorithm is given in the box below. That any policy with respect to is an improvement over any policy is assured by the policy improvement theorem. The proof can be found in page 101-102 in the book. Off-policy Methods (Off-policy Monte Carlo Prediction via Importance Sampling)Off-policy control methods has two policies on the same episode. One that is learned about and that becomes the optimal policy, called target policy, and one that is more exploratory and is used to generate behavior, called the behavior policy. In this case we say that learning is from data “off” the target policy, and the overall process is termed off-policy learning. Here, we only consider the prediction problem, in which both target and behavior policies are fixed. We require that implies , which is called the assumption of coverage, to assure that every action taken under is also taken under . It follows from coverage that must be stochastic in states where it is not identical to . The target policy may be deterministic. Importance samplingAlmost all off-policy methods utilize importance sampling, a general technique for estimating expected values under one distribution given samples from another. We apply importance sampling to off-policy learning by weighting returns according to the relative probability of their trajectories occurring under the target and behavior policies, called the importance-sampling ratio. Given a starting state , the probability of the subsequent state-action trajectory, , , , , , occurring under any policy is:where is the state-transition probability function. Thus, the relative probability of the trajectory under the target and behavior policies is:Since both policies depend on the same episodes, although we don’t know the exactly state transition probabilities, they are obviously identical. The importance sampling ratio ends up depending only on the two policies and the sequence, not on the MDP. Now. we have the returns from the behavior policy and its expectation:With importance sampling coming in, we can convert it to the right expectation on policy :With a batch of observed episodes, It is convenient to number time steps in a way that increases across episode boundaries. That is, if the first episode of the batch ends in a terminal state at time 100, then the next episode begins at time . We define the following symbols for the following formulas: denotes all steps that visit state s for an every-visit method, or the step of the first visit of state s for a first-visit method. denotes the first termination step (time) after time t. denotes the return from to . represents the corresponding importance-sampling ratio. With the above symbols, there are two representations of the value function Ordinary importance sampling Weighted importance sampling Formally, the difference between the first-visit methods of the two kinds of importance sampling is expressed in their biases and variances. bias variance Ordinary Importance Sampling unbiased unbounded because the variance of the ratios can be unbounded Weighted Importance Sampling biased bounded since the largest weight on any single return is one For the weighted importance sampling, the bias converges asymptotically to zero as the number of samples increases. The every-visit methods for ordinary and weighed importance sampling are both biased though the bias falls asymptotically to zero as the number of samples increases. In practice, every-visit methods are often preferred because they remove the need to keep track of which states have been visited and because they are much easier to extend to approximations. * The complete off-policy method on prediction problem is showed in the next section. Incremental ImplementationFor the on-policy method, just like it mentioned here, we average returns rather than rewards. For the off-policy method, in ordinary importance sampling, the returns are scaled by the importance sampling ratio and all other parts are the same as on-policy method. For the off-policy method, in weighted importance sampling, the algorithm is slightly different with the original one. Since we are talking about the value estimation of a one state, we use new symbols to simplify the formula. Here, we use denotes the value of the -th visited state , denotes , and denotes the cumulate sum of from . Then, where and is arbitrary. The complete algorithm is below: The algorithm is nominally for the off-policy case, using weighted importance sampling, but applies as well to the on-policy case just by choosing the target and behavior policies as the same (in which case (), is always 1). Off-policy Monte Carlo ControlThe algorithm is based on GPI and weighted importance sampling, for estimating and . A potential problem is that this method learns ONLY from the tails of episodes, when all of the remaining actions in the episode are greedy. If non-greedy actions are common, then learning will be slow, particularly for states appearing in the early portions of long episodes. If it is serious, the most important way to address it is probably by incorporating temporal-difference learning. Compare between two control methodsAll learning control methods face a dilemma: They seek to learn action values conditional on subsequent optimal behavior, but they need to behave non-optimally in order to explore all actions (to find the optimal actions). The on-policy approach is actually a compromise, it learns action values not for the optimal policy but for a near-optimal policy that still explores. The off-policy is more straightforward to it. It separate the policy to two policies and each one only does their job. For other parts: On-policy: On-policy methods are generally simpler and are considered first. Off-policy Off-policy methods are more powerful and general. They include on-policy methods as the special case in which the target and behavior policies are the same. Off-policy methods require additional concepts and notation, and because the data is due to a different policy, off-policy methods are often of greater variance and are slower to converge. Off-policy methods also have a variety of additional uses in applications. For example, they can often be applied to learn from data generated by a conventional non-learning controller, or from a human expert. Off-policy learning is also seen by some as key to learning multi-step predictive models of the world’s dynamics .","categories":[{"name":"Reinforcement Learning","slug":"Reinforcement-Learning","permalink":"https://lrscy.github.io/categories/Reinforcement-Learning/"}],"tags":[{"name":"Coursera","slug":"Coursera","permalink":"https://lrscy.github.io/tags/Coursera/"},{"name":"Reinforcement Learning","slug":"Reinforcement-Learning","permalink":"https://lrscy.github.io/tags/Reinforcement-Learning/"},{"name":"Notes","slug":"Notes","permalink":"https://lrscy.github.io/tags/Notes/"}]},{"title":"LeetCode 380 Insert Delete GetRandom O(1)","slug":"LeetCode-380-Insert-Delete-GetRandom-O(1)","date":"2020-06-28T23:24:30.000Z","updated":"2020-07-09T08:11:49.693Z","comments":true,"path":"2020/06/28/LeetCode-380-Insert-Delete-GetRandom-O(1)/","link":"","permalink":"https://lrscy.github.io/2020/06/28/LeetCode-380-Insert-Delete-GetRandom-O(1)/","excerpt":"","text":"CodeSolution of problem 380 is showed below. You can also find it here. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748auto ii &#x3D; []()&#123; ios_base::sync_with_stdio(false); cin.tie(NULL); cout.tie(NULL); return 0;&#125; ();class RandomizedSet &#123;public: &#x2F;** Initialize your data structure here. *&#x2F; RandomizedSet() &#123; &#125; &#x2F;** Inserts a value to the set. Returns true if the set did not already contain the specified element. *&#x2F; bool insert(int val) &#123; if(mp.count(val)) return false; v.push_back(val); mp[val] &#x3D; v.size() - 1; return true; &#125; &#x2F;** Removes a value from the set. Returns true if the set contained the specified element. *&#x2F; bool remove(int val) &#123; if(!mp.count(val)) return false; mp[v.back()] &#x3D; mp[val]; v[mp[val]] &#x3D; v.back(); v.pop_back(); mp.erase(val); return true; &#125; &#x2F;** Get a random element from the set. *&#x2F; int getRandom() &#123; return v[rand() % v.size()]; &#125;private: unordered_map&lt;int, int&gt; mp; vector&lt;int&gt; v;&#125;;&#x2F;** * Your RandomizedSet object will be instantiated and called as such: * RandomizedSet* obj &#x3D; new RandomizedSet(); * bool param_1 &#x3D; obj-&gt;insert(val); * bool param_2 &#x3D; obj-&gt;remove(val); * int param_3 &#x3D; obj-&gt;getRandom(); *&#x2F; SolutionThe hard point of the problem is the random operation. The insert and remove operator can be easily done by dictionary structure, like std:map or std:unordered_map. To implement the random operator, array-like stucture is necessary. But it is hard to add or delete in O(1) time. To combine them, we can save the data in a array-like structure and save their position in a dictionary structure. To remove a element, we can find the position from the dictionary structure and swap it with the last one so that we can remove it in both structure in O(1) time. VAluable MethodTo recall the problem, it is not that hard. But it took me long time on the random operator. It’s weird. When inserting or deleting elements in a heap, you add an element to the last position or swap the top element with the last element, and then follow the heap rules to adjust it. Here, it uses the same method.","categories":[{"name":"LeetCode","slug":"LeetCode","permalink":"https://lrscy.github.io/categories/LeetCode/"}],"tags":[{"name":"LeetCode","slug":"LeetCode","permalink":"https://lrscy.github.io/tags/LeetCode/"},{"name":"Solution","slug":"Solution","permalink":"https://lrscy.github.io/tags/Solution/"}]},{"title":"LeetCode 332 Reconstruct Itinerary","slug":"LeetCode-332-Reconstruct-Itinerary","date":"2020-06-28T21:28:49.000Z","updated":"2020-07-09T08:11:49.693Z","comments":true,"path":"2020/06/28/LeetCode-332-Reconstruct-Itinerary/","link":"","permalink":"https://lrscy.github.io/2020/06/28/LeetCode-332-Reconstruct-Itinerary/","excerpt":"","text":"CodeSolution of problem 332 is showed below. You can also find it here. 12345678910111213141516171819202122232425262728293031323334353637class Solution &#123;public: unordered_map&lt;string, vector&lt;pair&lt;string, bool&gt;&gt;&gt; ump; int tot &#x3D; 0; vector&lt;string&gt; findItinerary(vector&lt;vector&lt;string&gt;&gt;&amp; tickets) &#123; vector&lt;string&gt; ret; tot &#x3D; tickets.size(); if(!tot) return ret; ret.push_back(&quot;JFK&quot;); for(auto &amp;t: tickets) &#123; if(!ump.count(t[0])) ump[t[0]] &#x3D; vector&lt;pair&lt;string, bool&gt;&gt;(); ump[t[0]].push_back(&#123;t[1], false&#125;); &#125; for(auto it &#x3D; ump.begin(); it !&#x3D; ump.end(); ++it) &#123; sort(it-&gt;second.begin(), it-&gt;second.end()); &#125; dfs(ret, 0); return ret; &#125; bool dfs(vector&lt;string&gt; &amp;v, int n) &#123; if(n &#x3D;&#x3D; tot) return true; string s &#x3D; v.back(); if(!ump.count(s)) return false; for(auto it &#x3D; ump[s].begin(); it !&#x3D; ump[s].end(); ++it) &#123; if(!it-&gt;second) &#123; it-&gt;second &#x3D; true; v.push_back(it-&gt;first); if(dfs(v, n + 1)) return true; v.pop_back(); it-&gt;second &#x3D; false; &#125; &#125; return false; &#125;&#125;; Another one is from the web. 12345678910111213141516171819202122class Solution &#123;public: vector&lt;string&gt; findItinerary(vector&lt;pair&lt;string, string&gt;&gt; tickets) &#123; vector&lt;string&gt; res; stack&lt;string&gt; st&#123;&#123;&quot;JFK&quot;&#125;&#125;; unordered_map&lt;string, multiset&lt;string&gt;&gt; m; for (auto t : tickets) &#123; m[t.first].insert(t.second); &#125; while (!st.empty()) &#123; string t &#x3D; st.top(); if (m[t].empty()) &#123; res.insert(res.begin(), t); st.pop(); &#125; else &#123; st.push(*m[t].begin()); m[t].erase(m[t].begin()); &#125; &#125; return res; &#125;&#125;; SolutionIt is an example of Eulerian path finding. The first solution is just a regular DFS. It tries every possible way and save the current result. However, it obviously records more information it needs. The reason it needs to reset failed path is that it meets a dead end but not all edges are visited. But, there must exists an Eulerian path. Thus, if we meet a dead end, it tells us that we just need to visit other points and there must be a way to go back to the start point of the dead end path. As a result, we can record the dead end path and output them at last. It is the method of Hierholzer’s algorithm. The second solution is the Hierholzer’s algorithm. Valuable MethodIt is not hard to write the first method and also not hard to think about the second method even we don’t know Hierholzer’s algorithm before. If you know the Hierholzer’s algorithm, you can realize it immediately and solve the problem. But if not, just dig more from the Eulerian path rather than submitting a passable solution and you will learn more from the problem.","categories":[{"name":"LeetCode","slug":"LeetCode","permalink":"https://lrscy.github.io/categories/LeetCode/"}],"tags":[{"name":"LeetCode","slug":"LeetCode","permalink":"https://lrscy.github.io/tags/LeetCode/"},{"name":"Solution","slug":"Solution","permalink":"https://lrscy.github.io/tags/Solution/"}]},{"title":"LeetCode 476 Number Complement","slug":"LeetCode-476-Number-Complement","date":"2020-05-04T15:39:08.000Z","updated":"2020-07-09T08:11:49.693Z","comments":true,"path":"2020/05/04/LeetCode-476-Number-Complement/","link":"","permalink":"https://lrscy.github.io/2020/05/04/LeetCode-476-Number-Complement/","excerpt":"","text":"CodeSolution of problem 476 is showed below. You can also find it here. My solution123456789101112class Solution &#123;public: int findComplement(int num) &#123; int ret = 0, cnt = 0; while(num) &#123; ret |= ((num &amp; 1) ^ 1) &lt;&lt; cnt; num &gt;&gt;= 1; ++cnt; &#125; return ret; &#125;&#125;; A solution on Leetcode:123456789class Solution &#123;public: int findComplement(int num) &#123; int mask = 1; while(mask &lt; num) mask = (mask &lt;&lt; 1) + 1; return num ^ mask; &#125;&#125;; SolutionThis is a simple and easy problem. Just flip each bit and it’s all solved. You can iterate each bit or handle them all together. Valueable MethodWhen seeing this kind of problem, I used to handle each bit and assemble them together. However, vectorize them (create a number and XOR with the original number) is a better way. Also, I ignore the feature that the all 1 bits sequence is the largest number at the sequence length. Think more and code less all the time.","categories":[{"name":"LeetCode","slug":"LeetCode","permalink":"https://lrscy.github.io/categories/LeetCode/"}],"tags":[{"name":"LeetCode","slug":"LeetCode","permalink":"https://lrscy.github.io/tags/LeetCode/"},{"name":"Solution","slug":"Solution","permalink":"https://lrscy.github.io/tags/Solution/"}]},{"title":"Dynamic Programming","slug":"Coursera-Reinforcement-Learning-Course1-Week4-Notes","date":"2020-03-24T05:06:45.000Z","updated":"2020-07-09T08:11:49.681Z","comments":true,"path":"2020/03/24/Coursera-Reinforcement-Learning-Course1-Week4-Notes/","link":"","permalink":"https://lrscy.github.io/2020/03/24/Coursera-Reinforcement-Learning-Course1-Week4-Notes/","excerpt":"Fundamentals of Reinforcement Learning Week4 Notes","text":"Fundamentals of Reinforcement Learning Week4 Notes Policy Evaluation and Control Policy evaluation is a task of determining the state-value function , for a particular policy . It is also called prediction problem. Control is the task of improving an existing policy. it is also called policy improvement. Dynamic programming can be used to solve both problems, if we can get the dynamic function , which appears in all bellman functions about and . Policy EvaluationRecall the formula of , According to the above formula, it is a system of simultaneous linear equations in unknowns. However, in practice, due to the limitation about computation resource and time consuming, Dynamic Programming is always used to solve above problems. And iteration method is the most common used and suitable. To convert the above formula into iteration format, we just need to convert the note to iteration step indicator . Then we get the new formula, It is proved that the state-value function will be optimized. Moreover, there are two methods about implementing the iterate formula. One is to use two array to represent the state before and after one iteration. Another is to only use the original state array and update in-place. In practice, the second one provides us a faster converge speed, since it gets to use the updated values sooner, and occupies less memory space than the two array version implementation. Then the algorithm pseudo-code is: In practice, we almost cannot reach the convergence in short time, so that we set a margin to help us stop early and keep a relative good result. Policy ControlRecall that, If the greedification, the operation, doesn’t change , it means the is already greedy with respect to its value function . Moreover, we need to determine a way to compare two policies. The policy improvement theorem is introduced to do it. It says, Here, cannot be solved by linear programming. We also use dynamic programming and iteration method to solve the problem. In each iteration, according to the policy improvement theorem, we need to produce a strictly better policy than the original one. Generalized Policy IterationThe generalized policy iteration (GPI) refers to all the ways we can interleave policy evaluation and policy improvement. It can be showed as below, Policy IterationPolicy iteration is the process of finding an optimal policy by iteratively using policy evaluation and policy improvement (control). Such as, where denotes a policy evaluation and denotes a policy improvement. The whole process shows like, The algorithm pseudo-code is: Notes: Pros: Policy iteration often converges in surprisingly few iterations. Cons: each of the iteration involves policy evaluation, which may sweeps through the state set multiple times and consume lots of time before convergence. Value IterationValue iteration sweeps once in policy evaluation part and then do policy improvement. By combining the two formulas, the update formula would be, It can be showed as, For each iteration step, it doesn’t finish it completely but continuously improve itself until convergence. The algorithm pseudo-code is, Asynchronous Dynamic ProgrammingThe two iteration methods above are all synchronous DP, which means they systematically sweep the state set in one iteration. However, if the state set is very large, such as states, a single sweep can be prohibitively expensive. Asynchronous DP, instead, update values of states in any order. Some states may get several updates while other states are not visited. In order to guarantee convergence, asynchronous DP must continue to update the value of all states. Efficiency of Dynamic ProgrammingMonte Carlo methodThe key of Monte Carlo method is to calculate the mean of several samples under current state. The formula is, For each state, the method will average samples to get the value of the current state. When the number of state is large, it is time consuming. Brute-Force SearchBrute-Force Search is the most basic idea on finding the optimal policy. However, due to the number of possible state combination growth exponentially when states increase, , it is impossible to try them all. Dynamic ProgrammingCompare to Monte Carlo method, DP method use successors calculated state to calculate current state value, which do not handle each states independently. The method is also called bootstrapping. Also, Policy iteration can control the running time to polynomial time in and , rather than .","categories":[{"name":"Reinforcement Learning","slug":"Reinforcement-Learning","permalink":"https://lrscy.github.io/categories/Reinforcement-Learning/"}],"tags":[{"name":"Coursera","slug":"Coursera","permalink":"https://lrscy.github.io/tags/Coursera/"},{"name":"Reinforcement Learning","slug":"Reinforcement-Learning","permalink":"https://lrscy.github.io/tags/Reinforcement-Learning/"},{"name":"Notes","slug":"Notes","permalink":"https://lrscy.github.io/tags/Notes/"}]},{"title":"Finite Markov Decision Processess Notes (Cont.)","slug":"Coursera-Reinforcement-Learning-Course1-Week3-Notes","date":"2020-03-15T04:36:41.000Z","updated":"2020-07-09T08:11:49.681Z","comments":true,"path":"2020/03/15/Coursera-Reinforcement-Learning-Course1-Week3-Notes/","link":"","permalink":"https://lrscy.github.io/2020/03/15/Coursera-Reinforcement-Learning-Course1-Week3-Notes/","excerpt":"Fundamentals of Reinforcement Learning Week3 Notes","text":"Fundamentals of Reinforcement Learning Week3 Notes Policies and Value FunctionsPolicyPolicy is a mapping from states to actions. There are two kinds of policies: Deterministic PolicyIt can be represented in the form . Each state will lead to an action but not all actions have corresponding states. Stochastic PolicyIt can be formulized as , which is a probability distribution. Since it is a distribution, it should follows the following rules: Notes: A policy depends only on the current state. It is a restriction on the state rather than on the agent. Value FunctionValue function is designed to evaluate expected return in the future in a given state. It aggregates many possible future returns into a single number. State-value functionThe value function of a state under a policy denoted , is the expected return when starting in and following thereafter. For MDPs, we can define formally by: The value of any terminal state is 0. We call the function the state-value function for policy . Action-value functionSimilarly, we define the value of taking action in state under a policy , denoted , as the expected return starting from , taking the action , and thereafter following policy : We call the function the action-value function for policy . Value functions are crucial in reinforcement learning. They allow an agent to query the quality of its current situation without waiting to observe the long-term outcome. The benefits is twofold: The return is immediately available The return may be random due to stochastic in both the policy and environment dynamics Bellman FunctionBellman function is used to formalize the connection between the value of and the value of its possible successor states. State-value Bellman Function Action-value Bellman Function Summary Bellman functions provides us a way to solve the value function by writing a system of linear equations. It can be only directly used on small MDPs Optimal Policies and Optimal Value FunctionsAn optimal policy is defined as the policy with the highest possible value function in all states. There is at least one exist. It can be concatenated by best parts of multiple policies. Due to the exponential number of possible policies, brute-force searching is not impossible. We always use and to denote optimal state-value function and optimal action-value function respectively. Bellman Optimal Equation for According to the formula , Since the optimal policy can be made of optimal action in every state, the would be for the optimal action, achieve the highest value, and for else, which means it becomes a deterministic optimal policy. Then, it becomes Bellman Optimal Equation for Similarly, we can do it on . Notes: Since and already take into account the reward consequence of all possible future behavior, the greedy method here will finally lead the the final optimal result. We cannot simply use linear system solver to solve the optimal because it is the thing we would like to solve. Determine an Optimal PolicyFor , If we have , we can get easier,","categories":[{"name":"Reinforcement Learning","slug":"Reinforcement-Learning","permalink":"https://lrscy.github.io/categories/Reinforcement-Learning/"}],"tags":[{"name":"Coursera","slug":"Coursera","permalink":"https://lrscy.github.io/tags/Coursera/"},{"name":"Reinforcement Learning","slug":"Reinforcement-Learning","permalink":"https://lrscy.github.io/tags/Reinforcement-Learning/"},{"name":"Notes","slug":"Notes","permalink":"https://lrscy.github.io/tags/Notes/"}]},{"title":"Finite Markov Decision Processes Notes","slug":"Coursera-Reinforcement-Learning-Course1-Week2-Notes","date":"2020-03-03T05:12:59.000Z","updated":"2020-07-09T08:11:49.681Z","comments":true,"path":"2020/03/03/Coursera-Reinforcement-Learning-Course1-Week2-Notes/","link":"","permalink":"https://lrscy.github.io/2020/03/03/Coursera-Reinforcement-Learning-Course1-Week2-Notes/","excerpt":"Fundamentals of Reinforcement Learning Week2 Notes","text":"Fundamentals of Reinforcement Learning Week2 Notes MDPs are a classical formalization of sequential decision making, where actions influence not just immediate rewards, but also subsequent situations, or states, and through those future rewards. Thus MDPs involve delayed reward and the need to tradeoff immediate and delayed reward. Whereas in bandit problems we estimated the value of each action , in MDPs we estimate the value of each action in each state , or we estimate the value of each state given optimal action selections. The Agent-Environment InterfaceGeneral Definition The learner and decision maker is called the agent. The thing it interacts with, comprising everything outside the agent, is called the environment. At time , the agent receive some representation of the environment , , and on the basis select an , . One time step later, the agent receive a , , from the environment and find itself in a new , . With time goes one, the sequence would be In MDP, the , , all has finite number of elements. The whole process can also be defined as where , , . The function defines the dynamics of the MDP. The dynamics function : is an ordinary deterministic function of four arguments. And it follows the rules below: In MDP, for current and , they only depend on the previous and . This is best viewed as a restriction not on the decision process, but on the . The state must include information about all aspects of the past agent–environment interaction that make a difference for the future. If it does, then the state is said to have the Markov property. Common Used FormulasThe formula (2) is the most common used. There are also three important formulas used in solving MDP problems for convenience. The first one is the state transaction probabilities as a three augments function : , The second one is the expect rewards for state-action pairs as a two augments function : , The third one is the expect rewards for state-action-next-state triples as a three augments function : , Boundaries of Agent and EnvironmentThe MDP framework is abstract and flexible and can be applied to many different problems in several ways, containing both high-level and low level controls, states, etc. The boundary of agent and environment is different from the physical reality. The boundary represents the limit of the agent’s absolute control, not of its knowledge. The general rule we follow is that anything that cannot be changed arbitrarily by the agent is considered to be outside of it and thus part of its environment. The MDP framework is a considerable abstraction of the problem of goal-directed learning from interaction. For any goal-directed problem, it can be reduced to the three signals: , , and . Goals and RewardsAt each time step , the reward is a simple number, . Informally, the agent’s goal is to maximize the total amount of reward it receives, in other word, the cumulative reward in the long run, rather than maximizing immediate reward. The use of a reward signal to formalize the idea of a goal is one of the most distinctive features of reinforcement learning. Although formulating goals in terms of reward signals might at first appear limiting, in practice it has proved to be flexible and widely applicable. The reward signal is your way of communicating to the robot what you want it to achieve, not how you want it achieved. Returns and EpisodesEpisodic TasksFeatures of this kind of tasks. interaction breaks naturally into episodes Each episode ends in a terminal state Episodes are independent. In some tasks, natural agent-environment interactions sequence can be broken into several sub-sequences, which is also called episode. For example, when you play a game, your interaction with game is natural and non-interrupted. But when you win or lose a game, you reach a final state of a game and the actions before the end is a sub-sequence. To formulize the long run cumulative rewards, from next time to the final state , we can simply sum them up or set weights for future rewards. The formula is as follows, where is the final time step. Continuing TasksIn contrast to episodic tasks, this kind of task goes on continually and no terminal state. To formulize it, One common way is to use discounting. The total long run reward, called discounted return, where is the discount rate, . , the agent is “myopic” and only concerns to maximize immediate rewards, which may reduce the final return. , we will get a result as long as the reward sequence is finite. , the agent becomes more farsighted. If the reward is nonzero and constant and , the total reward is, If we can assume for , and , the total reward would be, Where is finite. We can also write the formula above into another form, where .","categories":[{"name":"Reinforcement Learning","slug":"Reinforcement-Learning","permalink":"https://lrscy.github.io/categories/Reinforcement-Learning/"}],"tags":[{"name":"Coursera","slug":"Coursera","permalink":"https://lrscy.github.io/tags/Coursera/"},{"name":"Reinforcement Learning","slug":"Reinforcement-Learning","permalink":"https://lrscy.github.io/tags/Reinforcement-Learning/"},{"name":"Notes","slug":"Notes","permalink":"https://lrscy.github.io/tags/Notes/"}]},{"title":"The K-Armed Bandit Problem Notes","slug":"Coursera-Reinforcement-Learning-Course1-Week1-Notes","date":"2020-03-03T05:12:51.000Z","updated":"2020-07-09T08:11:49.681Z","comments":true,"path":"2020/03/03/Coursera-Reinforcement-Learning-Course1-Week1-Notes/","link":"","permalink":"https://lrscy.github.io/2020/03/03/Coursera-Reinforcement-Learning-Course1-Week1-Notes/","excerpt":"Fundamentals of Reinforcement Learning Week1 Notes","text":"Fundamentals of Reinforcement Learning Week1 Notes First of all, there are some notations need to be determined. Notation Explanation number of actions (arms) discrete time step or play number true value (expected reward) of action estimate at time of number of times action has been selected up prior to time learned preference for selecting action at time probability of selecting action at time estimate at time of the expected reward given DefinitionConsider the following learning problem. You are faced repeatedly with a choice among k different options, or actions. After each choice you receive a numerical reward chosen from a stationary probability distribution that depends on the action you selected. Your objective is to maximize the expected total reward over some time period, for example, over 1000 action selections, or time steps. This is the original form of the k-armed bandit problem, so named by analogy to a slot machine, or “one-armed bandit”, except that it has k levers instead of one. Each action selection is like a play of one of the slot machine’s levers, and the rewards are the payoffs for hitting the jackpot. Through repeated action selections you are to maximize your winnings by concentrating your actions on the best levers. In short, we need to calculate the following formula: To explore the best result, there are two types of actions, exploitation and exploration. Exploitation means selecting the greedy action so that you can gain high reward. Exploration means selecting the nongreedy action in which you may improve your estimate value. MethodAction-value MethodThis is the most simple and straight one. We calculate value of action using following formula: When , converges to . We call this the sample-average. When , we may set . Then, the selection rule is greedy selection, as the following: For those equal valued , we just arbitrarily select one. This step is called exploiting. To explore other actions which may improve the result, a simple strategy called is used. For each action, with a small probability , we select to explore, to randomly select an action among all actions. With probability , we do exploitation normally. Say short, the test result shows that method outperformed than pure greedy method, when . Incremental ImplementationAt time , we select one action -th time. The estimate value of the -th action is: We can store the sum of , add each new reward on it, and then calculate the current value. We only need to store and . Also we can see it in an iteration format. To generalize it, the form is: The is an error in the estimation. And . It has a sense of similarity with the gradient descent. Finally, a simple bandit algorithm is as the following: Initialize, for a = 1 to k: Loop: Tracking a Nonstationary ProblemIn reinforcement learning, a common situation is that we give more weight to recent rewards than to long-past rewards. When we assign an value . The formula can be transformed to the following: We call it a weighted average because the sum of the weights is , which is also called exponential recency-weighted average. Let’s use denoting the step-size parameter, which is a list of sequence with no length limitation. The following is the condition: The first condition is required to guarantee that the steps are large enough to eventually overcome any initial conditions or random fluctuations. The second condition guarantees that eventually the steps become small enough to assure convergence. Note: Sample-average, , meets all conditions. If , a constant, it will not meet the second condition and will never completely converge but continue to vary in response to the most recently received rewards. The sequence of step-size parameters that meets formular conditions are often very slow. It is often used in theoretical work but not in applications and empirical research. OptimizationOptimistic Initial ValuesIn above formula , the selection of can affect the converge speed and accuracy. We can set , which is wildly optimistic initial value. With such value, each reward of the following value will be “unsatisfied”, so that it will go to explore. We call the technique optimistic initial values. The technique is quite effective on stationary problems, but not on nonstationary problems. The reason is that it always start from the beginning. If the task changes, it needs to learn from the start and do exploration. Also, we don’t know the maximal reward. Upper-Confidence-Bound Action SelectionIn the former formulas, we all discuss about how to get a better , exploitation, value but ignore how to explore more efficiently. One effective way of doing this is to select actions according to: denotes the number of times that action a has been selected prior to time t. controls the degree of exploration. If , then a is considered to be a maximizing action. This is called upper confidence bound (UCB). The experiment results shows that it performs better than -greedy on the problem. However, there are several reasons why it is usually not pratical in other reinforcement learning. It is more complex than -greedy method. It is hard to deal with large state spaces. In short, it is specific designed for the k-armed bandit problem.","categories":[{"name":"Reinforcement Learning","slug":"Reinforcement-Learning","permalink":"https://lrscy.github.io/categories/Reinforcement-Learning/"}],"tags":[{"name":"Coursera","slug":"Coursera","permalink":"https://lrscy.github.io/tags/Coursera/"},{"name":"Reinforcement Learning","slug":"Reinforcement-Learning","permalink":"https://lrscy.github.io/tags/Reinforcement-Learning/"},{"name":"Notes","slug":"Notes","permalink":"https://lrscy.github.io/tags/Notes/"}]},{"title":"Dialog Response Prediction Related Paper Read","slug":"Dialog-Response-Prediction","date":"2019-02-12T01:40:58.000Z","updated":"2020-07-09T08:11:49.693Z","comments":true,"path":"2019/02/11/Dialog-Response-Prediction/","link":"","permalink":"https://lrscy.github.io/2019/02/11/Dialog-Response-Prediction/","excerpt":"Multi-turn Question-Answering/Chatbot problems are one of the hottest topics in NLP field recently. The blog sum up some tradition papers and several recent papers.","text":"Multi-turn Question-Answering/Chatbot problems are one of the hottest topics in NLP field recently. The blog sum up some tradition papers and several recent papers. Multi-view Response Selection for Human-Computer ConversationTags: multi views, general attention, disagreement loss, likelihood loss The structure of the model is clear. The essence of the model in my view is multi-view and multi-loss. By applying two levels (context level and sentence level) information extractor, the model can gather two independent view of the whole context. Also, two loss functions can be seen as two views on selecting answers. The only drawback of the model, in my view, is that the matching operation comes too late, just at the end of the model. Sequential Matching Network A New Architecture for Multi-turn Response Selection in Retrieval-Based Chatbots (SMN)Tags: multi level info, dot self attention, CNN filter The model’s structure is basically clear. Just mention that the word pairs matrix comes from word embeddings rather than GRU which is the source of segment pair matrix. In the last layer, the author tried three methods for the L(.) function, the attention combination method works best, linear combination works worst, and last hidden state method works fair but fast. The author also use multi-view info to extract information from sentences, like what the previous paper did.","categories":[{"name":"NLP","slug":"NLP","permalink":"https://lrscy.github.io/categories/NLP/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"https://lrscy.github.io/tags/NLP/"},{"name":"DeepLearning","slug":"DeepLearning","permalink":"https://lrscy.github.io/tags/DeepLearning/"}]},{"title":"LeetCode 984 String Without AAA or BBB","slug":"LeetCode-984-String-Without-AAA-or-BBB","date":"2019-01-27T20:18:19.000Z","updated":"2020-07-09T08:11:49.693Z","comments":true,"path":"2019/01/27/LeetCode-984-String-Without-AAA-or-BBB/","link":"","permalink":"https://lrscy.github.io/2019/01/27/LeetCode-984-String-Without-AAA-or-BBB/","excerpt":"","text":"CodeThe solution of problem 984 is showed below. You can also find it here. 12345678910111213class Solution {public: string strWithout3a3b(int A, int B) { string s = \"\"; int a = 0, b = 0; while( A || B ) { int ta = 0, tb = 0; while( ta &lt; 2 &amp;&amp; B &lt;= 2 * A &amp;&amp; A &gt; 0 ) { s += \"a\"; ++ta; --A; } while( tb &lt; 2 &amp;&amp; A &lt;= 2 * B &amp;&amp; B &gt; 0 ) { s += \"b\"; ++tb; --B; } } return s; }}; SolutionThe core method of the problem is to make sure that the number of each character is no less than half of another character and no exceed two consecutively. Valuable MethodI’ve tried to find pattern at first time, such as “aab”, “abb”, and etc. Finally, I find that if we want to prevent those “aaa” and “bbb” appear, we just need to keep the ratio of number of two characters. It’s easier to apply a simple rule rather than setting many patterns and it looks clean and beautiful.","categories":[{"name":"LeetCode","slug":"LeetCode","permalink":"https://lrscy.github.io/categories/LeetCode/"}],"tags":[{"name":"LeetCode","slug":"LeetCode","permalink":"https://lrscy.github.io/tags/LeetCode/"},{"name":"Solution","slug":"Solution","permalink":"https://lrscy.github.io/tags/Solution/"}]},{"title":"LeetCode 982 Triples with Bitwise AND Equal To Zero","slug":"LeetCode-982-Triples-with-Bitwise-AND-Equal-To-Zero","date":"2019-01-27T19:19:55.000Z","updated":"2020-07-09T08:11:49.693Z","comments":true,"path":"2019/01/27/LeetCode-982-Triples-with-Bitwise-AND-Equal-To-Zero/","link":"","permalink":"https://lrscy.github.io/2019/01/27/LeetCode-982-Triples-with-Bitwise-AND-Equal-To-Zero/","excerpt":"","text":"CodeThe solution of problem 982 is showed below. You can also find it here. 12345678910111213141516171819class Solution &#123;public: int countTriplets(vector&lt;int&gt;&amp; A) &#123; int n &#x3D; A.size(); int inv[1 &lt;&lt; 16] &#x3D; &#123; 0 &#125;; for( auto a : A ) &#123; for( int i &#x3D; 0; i &lt; ( 1 &lt;&lt; 16 ); ++i ) if( ( a &amp; i ) &#x3D;&#x3D; 0 ) ++inv[i]; &#125; int ret &#x3D; 0; for( int i &#x3D; 0; i &lt; n; ++i ) &#123; for( int j &#x3D; 0; j &lt; n; ++j ) &#123; ret +&#x3D; inv[A[i] &amp; A[j]]; &#125; &#125; return ret; &#125;&#125;; SolutionThe core method to solve it is to treat one number previously and traverse all combination of two numbers $a \\&amp; b$ in the vector. In this way, we can reduce the time complexity from $O(N^3)$ to $O(N^2)$. The treatment is to pre-calculate each number $a$ in the vector to find with which number $b$ $a \\&amp; b$ would get $0$. Valuable MethodThe pre-treating method is really important when solving problems. If we enumerate all three numbers combination, for example $a \\&amp; b \\&amp; c$, we would repeatly calculate &amp; operator with $c$ for many times. By remembering it, we could use more space to reduce run time. Moreoever, we could also store the result of $a \\&amp; b$ and check the result by $c$.","categories":[{"name":"LeetCode","slug":"LeetCode","permalink":"https://lrscy.github.io/categories/LeetCode/"}],"tags":[{"name":"LeetCode","slug":"LeetCode","permalink":"https://lrscy.github.io/tags/LeetCode/"},{"name":"Solution","slug":"Solution","permalink":"https://lrscy.github.io/tags/Solution/"}]},{"title":"LeetCode 843 Guess the Word","slug":"LeetCode-843-Guess-the-Word","date":"2019-01-27T18:54:15.000Z","updated":"2020-07-09T08:11:49.693Z","comments":true,"path":"2019/01/27/LeetCode-843-Guess-the-Word/","link":"","permalink":"https://lrscy.github.io/2019/01/27/LeetCode-843-Guess-the-Word/","excerpt":"","text":"CodeThe solution of problem 843 is showed below. You can also find it here. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960&#x2F;** * &#x2F;&#x2F; This is the Master&#39;s API interface. * &#x2F;&#x2F; You should not implement it, or speculate about its implementation * class Master &#123; * public: * int guess(string word); * &#125;; *&#x2F;auto desyncio &#x3D; []()&#123; std::ios::sync_with_stdio(false); cin.tie(nullptr); return nullptr;&#125;();class Solution &#123;public: void findSecretWord(vector&lt;string&gt;&amp; wordlist, Master&amp; master) &#123; long long prob[10][30] &#x3D; &#123; 0 &#125;; for( auto &amp;w : wordlist ) &#123; for( int i &#x3D; 0; i &lt; 6; ++i ) prob[i][w[i] - &#39;a&#39;] +&#x3D; 1; &#125; while( true ) &#123; string tmp &#x3D; generate( prob, wordlist ); int ret &#x3D; master.guess( tmp ); if( ret &#x3D;&#x3D; tmp.length() ) break; for( auto it &#x3D; wordlist.begin(); it !&#x3D; wordlist.end(); ) &#123; if( match( *it, tmp ) !&#x3D; ret ) &#123; for( int i &#x3D; 0; i &lt; 6; ++i ) prob[i][( *it )[i] - &#39;a&#39;] -&#x3D; 1; wordlist.erase( it ); &#125; else ++it; &#125; &#125; return ; &#125; string generate( long long prob[][30], vector&lt;string&gt;&amp; wordlist ) &#123; string ret; long long best &#x3D; 0; for( auto w : wordlist ) &#123; long long tpscore &#x3D; 1; for( int i &#x3D; 0; i &lt; 6; ++i ) tpscore *&#x3D; prob[i][w[i] - &#39;a&#39;]; if( tpscore &gt; best ) &#123; best &#x3D; tpscore; ret &#x3D; w; &#125; &#125; return ret; &#125; int match( string a, string b ) &#123; int ret &#x3D; 0; for( int i &#x3D; 0; i &lt; a.length(); ++i ) &#123; if( a[i] &#x3D;&#x3D; b[i] ) ++ret; &#125; return ret; &#125;&#125;; SolutionThe thought of the problem is from a discussion of the problem. Basically, we count all characters on each position from 1 to 6 as there probability. Then we generate a string with highest probability and guess it. Based on what return to us, we eliminate all other candidates which does not meet the requirement. Finally, we will guess the right word. Interesting PointIt’s a whole new kind of problem. The method to solve it is now fixed. Statistic is a candidate method to this problem. This kind of problem is just so fun and interesting!","categories":[{"name":"LeetCode","slug":"LeetCode","permalink":"https://lrscy.github.io/categories/LeetCode/"}],"tags":[{"name":"LeetCode","slug":"LeetCode","permalink":"https://lrscy.github.io/tags/LeetCode/"},{"name":"Solution","slug":"Solution","permalink":"https://lrscy.github.io/tags/Solution/"}]},{"title":"LeetCode 200 Number of Islands","slug":"LeetCode-200-Number-of-Islands","date":"2019-01-27T18:24:09.000Z","updated":"2020-07-09T08:11:49.693Z","comments":true,"path":"2019/01/27/LeetCode-200-Number-of-Islands/","link":"","permalink":"https://lrscy.github.io/2019/01/27/LeetCode-200-Number-of-Islands/","excerpt":"","text":"CodeThe solution of problem 200 is showed below. You can also find it here. 1234567891011121314151617181920212223242526272829303132333435363738394041424344auto desyncio &#x3D; []()&#123; std::ios::sync_with_stdio(false); cin.tie(nullptr); return nullptr;&#125;();class Solution &#123;public: int dx[4] &#x3D; &#123; 1, 0, -1, 0 &#125;, dy[4] &#x3D; &#123; 0, 1, 0, -1 &#125;; int nx, ny; void bfs( vector&lt;vector&lt;char&gt;&gt;&amp; grid, int x, int y ) &#123; queue&lt;pair&lt;int, int&gt;&gt; q; q.push( make_pair( x, y ) ); grid[x][y] &#x3D; &#39;0&#39;; while( !q.empty() ) &#123; auto tmp &#x3D; q.front(); q.pop(); for( int i &#x3D; 0; i &lt; 4; ++i ) &#123; int tx &#x3D; tmp.first + dx[i], ty &#x3D; tmp.second + dy[i]; if( tx &lt; 0 || tx &gt;&#x3D; nx || ty &lt; 0 || ty &gt;&#x3D; ny ) continue; if( grid[tx][ty] &#x3D;&#x3D; &#39;1&#39; ) &#123; grid[tx][ty] &#x3D; &#39;0&#39;; q.push( make_pair( tx, ty ) ); &#125; &#125; &#125; &#125; int numIslands(vector&lt;vector&lt;char&gt;&gt;&amp; grid) &#123; int ret &#x3D; 0; nx &#x3D; grid.size(); if( nx ) ny &#x3D; grid[0].size(); for( int i &#x3D; 0; i &lt; nx; ++i ) &#123; for( int j &#x3D; 0; j &lt; ny; ++j ) &#123; if( grid[i][j] &#x3D;&#x3D; &#39;1&#39; ) &#123; ++ret; bfs( grid, i, j ); &#125; &#125; &#125; return ret; &#125;&#125;; SolutionThe solution of the problem is simply BFS/DFS. Check on each position and go BFS/DFS search when the position is “1”. Valueable MethodI choose the BFS and here is a trick. If you choose to change status after retreat from queue, you will push a huge amount of duplicate position into queue which will cause “Memory Limit Exceed” and slow down your program. So If treating each action before pushing into queue, it would save huge amount of memory and run faster.","categories":[{"name":"LeetCode","slug":"LeetCode","permalink":"https://lrscy.github.io/categories/LeetCode/"}],"tags":[{"name":"LeetCode","slug":"LeetCode","permalink":"https://lrscy.github.io/tags/LeetCode/"},{"name":"Solution","slug":"Solution","permalink":"https://lrscy.github.io/tags/Solution/"}]},{"title":"LeetCode 159 Longest Substring with At Most Two Distinct Characters","slug":"LeetCode-159-Longest-Substring-with-At-Most-Two-Distinct-Characters","date":"2019-01-27T16:47:14.000Z","updated":"2020-07-09T08:11:49.693Z","comments":true,"path":"2019/01/27/LeetCode-159-Longest-Substring-with-At-Most-Two-Distinct-Characters/","link":"","permalink":"https://lrscy.github.io/2019/01/27/LeetCode-159-Longest-Substring-with-At-Most-Two-Distinct-Characters/","excerpt":"","text":"CodeThe solution of problem 159 is showed below. You can also find it here. 1234567891011121314151617181920212223242526272829class Solution &#123;public: int lengthOfLongestSubstringTwoDistinct(string s) &#123; int ret &#x3D; 0, len &#x3D; s.length(); int i &#x3D; 0, j &#x3D; 0; unordered_map&lt;int, int&gt; mp; if( s.size() &lt;&#x3D; 2 ) return s.size(); while( j &lt; len ) &#123; mp[s[j]] &#x3D; j; if( mp.size() &gt; 2 ) &#123; ret &#x3D; max( ret, j - i ); while( mp.size() &gt; 2 ) &#123; int key, n &#x3D; INT_MAX; for( auto it &#x3D; mp.begin(); it !&#x3D; mp.end(); ++it ) &#123; if( it-&gt;second &lt; n ) &#123; n &#x3D; it-&gt;second; key &#x3D; it-&gt;first; &#125; &#125; i &#x3D; n + 1; mp.erase( key ); &#125; &#125; ++j; &#125; ret &#x3D; max( ret, j - i ); return ret; &#125;&#125;; SolutionTo solve the problem, you need to build a map to store the last position of each unique element and use sliding window to get the final result. The sliding window means that expanding the right side of the window first until exceeding the limit, three unique elements, then querying each element in map and kicking the most left element out. In this way, the window starts sliding until the end of the string. Valuable MethodAt the first time, I use map to store number of elements in a sliding window rather than their position. It’s OK but stores an useless information–number. Also, my first try will scan the whole string twice which slows down the whole program. However, if we just store the position of each element, we just need to scan the whole string once and also get the right result. It’s also a kind of optimization on constant, which reduces the time complexity from O(2N) to O(N).","categories":[{"name":"LeetCode","slug":"LeetCode","permalink":"https://lrscy.github.io/categories/LeetCode/"}],"tags":[{"name":"LeetCode","slug":"LeetCode","permalink":"https://lrscy.github.io/tags/LeetCode/"},{"name":"Solution","slug":"Solution","permalink":"https://lrscy.github.io/tags/Solution/"}]},{"title":"LeetCode 309 Best Time to Buy and Sell Stock with Cooldown","slug":"LeetCode-309-Best-Time-to-Buy-and-Sell-Stock-with-Cooldown","date":"2019-01-27T15:52:23.000Z","updated":"2020-07-09T08:11:49.693Z","comments":true,"path":"2019/01/27/LeetCode-309-Best-Time-to-Buy-and-Sell-Stock-with-Cooldown/","link":"","permalink":"https://lrscy.github.io/2019/01/27/LeetCode-309-Best-Time-to-Buy-and-Sell-Stock-with-Cooldown/","excerpt":"","text":"CodeSolution of problem 309 is showed below. You can also find it here. 1234567891011121314151617181920class Solution &#123;public: int maxProfit(vector&lt;int&gt;&amp; prices) &#123; if(prices.size() &lt; 2) return 0; &#x2F;&#x2F;s0 denote maxprofit at i-th day without stock &#x2F;&#x2F;s1 denote maxprofit at i-th day with stock &#x2F;&#x2F;s2 denote maxprofit at i-th day when cooling down. &#x2F;&#x2F;s0 could be transited from s0 at i-1th day (do nothing), or from s1 at i-1th day (sell) &#x2F;&#x2F;s1 could be transited from s1 at i-1th day (do nothing), or from s2 at i-1th day (cool to buy) &#x2F;&#x2F;s2 could be transited from s2 at i-1th day (do nothing), or from s0 at i-1th day (do nothing) int s0 &#x3D; 0, s1 &#x3D; -1e9, s2 &#x3D; 0; for(const auto &amp;p: prices) &#123; int tmp &#x3D; s0; s0 &#x3D; max(s0, s1 + p); s1 &#x3D; max(s1, s2 - p); s2 &#x3D; max(s2, tmp); &#125; return max(s0, s2); &#125;&#125;; SolutionThe method of the solution is from Discuss. There is another method which use operations rather than states to record status and do calculation. Valueable FunctionThe interest part of the first solution is that it uses state machine and minimizes the memory usage by just storing constant level variables. However, it can only be applied in the situation. If the problem said you can only buy stock two days later after selling it, the solution cannot hold it. So the second solution is versatility.","categories":[{"name":"LeetCode","slug":"LeetCode","permalink":"https://lrscy.github.io/categories/LeetCode/"}],"tags":[{"name":"LeetCode","slug":"LeetCode","permalink":"https://lrscy.github.io/tags/LeetCode/"},{"name":"Solution","slug":"Solution","permalink":"https://lrscy.github.io/tags/Solution/"}]},{"title":"LeetCode 416 Partition Equal Subset Sum","slug":"LeetCode-416-Partition-Equal-Subset-Sum","date":"2019-01-27T15:52:23.000Z","updated":"2020-07-09T08:11:49.693Z","comments":true,"path":"2019/01/27/LeetCode-416-Partition-Equal-Subset-Sum/","link":"","permalink":"https://lrscy.github.io/2019/01/27/LeetCode-416-Partition-Equal-Subset-Sum/","excerpt":"","text":"CodeSolution of problem 416 is showed below. You can also find it here. 1234567891011121314151617181920static int desyncio &#x3D; []() &#123; std::ios::sync_with_stdio( false ); cin .tie( nullptr ); cout.tie( nullptr ); return 0;&#125;();class Solution &#123;public: bool canPartition(vector&lt;int&gt;&amp; nums) &#123; bitset&lt;20010&gt; bs(1); int sum &#x3D; 0; for(auto n: nums) sum +&#x3D; n; if(sum &amp; 1) return false; for(auto n: nums) &#123; bs |&#x3D; (bs &lt;&lt; n); &#125; return bs[sum &gt;&gt;&#x3D; 1]; &#125;&#125;; SolutionThere are two solutions. The core method of both solutions is dp, specifically 0-1 Knapsack problem. The first solution uses “bitset” to storage status. The second solution, which doesn’t show here since it’s too simple and you need to implement it by yourself, uses “bool array” to storage all status. The core operation (at line 16) of two solutions is different due to feature of the data structure they use. According to the result on LeetCode, the second solution is slower than the first one. In my opinion, bit operations is much faster than scanning through bool array. Valueable FunctionIt is a good problem to practice “bitset” data structure. It is made of bits and can store more than 64 bits, which is the length of long long or unsigned long long. It also implements almost all bit operations.","categories":[{"name":"LeetCode","slug":"LeetCode","permalink":"https://lrscy.github.io/categories/LeetCode/"}],"tags":[{"name":"LeetCode","slug":"LeetCode","permalink":"https://lrscy.github.io/tags/LeetCode/"},{"name":"Solution","slug":"Solution","permalink":"https://lrscy.github.io/tags/Solution/"}]},{"title":"LeetCode 146 LRU Cache","slug":"LeetCode-146-LRU-Cache","date":"2019-01-27T15:52:23.000Z","updated":"2020-07-09T08:11:49.693Z","comments":true,"path":"2019/01/27/LeetCode-146-LRU-Cache/","link":"","permalink":"https://lrscy.github.io/2019/01/27/LeetCode-146-LRU-Cache/","excerpt":"","text":"CodeSolution of problem 146 is showed below. You can also find it here. 1234567891011121314151617181920212223242526272829303132333435363738394041424344static const int _ &#x3D; []() &#123; ios::sync_with_stdio(false); cin.tie(nullptr); return 0;&#125;();class LRUCache &#123;public: LRUCache(int capacity) &#123; cap &#x3D; capacity; &#125; int get(int key) &#123; if( !mp.count( key ) ) return -1; lst.splice( lst.begin(), lst, mp[key] ); return mp[key]-&gt;second; &#125; void put(int key, int value) &#123; if( mp.count( key ) ) &#123; mp[key]-&gt;second &#x3D; value; lst.splice( lst.begin(), lst, mp[key] ); &#125; else &#123; lst.push_front( make_pair( key, value ) ); mp[key] &#x3D; lst.begin(); if( lst.size() &gt; cap ) &#123; mp.erase( lst.rbegin()-&gt;first ); lst.pop_back(); &#125; &#125; &#125; private: int cap; unordered_map&lt;int, list&lt;pair&lt;int, int&gt;&gt;::iterator&gt; mp; list&lt;pair&lt;int, int&gt;&gt; lst;&#125;;&#x2F;** * Your LRUCache object will be instantiated and called as such: * LRUCache obj &#x3D; new LRUCache(capacity); * int param_1 &#x3D; obj.get(key); * obj.put(key,value); *&#x2F; SolutionTo solve the problem, you need to build a list to store the order of “key”-“value” pair and to build a map to store “key”-“position in list” pair. Thus, when putting a “key”-“value” pair, you will easily know its position and update the order. Valueable FunctionThe interesting part of this code is the function “splice” of list. It can be found on c++ reference.1void splice (iterator position, list&amp; x, iterator i);The function will transfer element “i” in list “x” to specific “position”. Before using this funcition, I wrote a segment of code to implement the function by myself but it runs slower than the function. In this case, by using this function, I transfer the “mp[key]” in list “lst” to the front of the list “lst”.","categories":[{"name":"LeetCode","slug":"LeetCode","permalink":"https://lrscy.github.io/categories/LeetCode/"}],"tags":[{"name":"LeetCode","slug":"LeetCode","permalink":"https://lrscy.github.io/tags/LeetCode/"},{"name":"Solution","slug":"Solution","permalink":"https://lrscy.github.io/tags/Solution/"}]},{"title":"LeetCode Solution Summary","slug":"LeetCode","date":"2019-01-27T15:05:53.000Z","updated":"2020-07-09T08:11:49.693Z","comments":true,"path":"2019/01/27/LeetCode/","link":"","permalink":"https://lrscy.github.io/2019/01/27/LeetCode/","excerpt":"This blog is a summary of my solution of LeetCode problems. I will share several interesting problems in this blog and category. Most of my solutions are on my Github. Most of my algorithm problem solutions are written in C++ and some of them are written in Python3. Most of my databse problem solutions are written in MySQL.","text":"This blog is a summary of my solution of LeetCode problems. I will share several interesting problems in this blog and category. Most of my solutions are on my Github. Most of my algorithm problem solutions are written in C++ and some of them are written in Python3. Most of my databse problem solutions are written in MySQL. Basic AccelerationHere is a way to reduce runtime when judging online with C++. Here is the code: 123456static int desyncio = []() &#123; std::ios::sync_with_stdio( false ); cin .tie( nullptr ); cout.tie( nullptr ); return 0;&#125;(); This code will close the synchronize of two kinds of input and output in C format and C++ format. In C, mostly, we use “scanf”, “printf”, and other similar functions to control the input and output of our program, while in C++, we use “cin”, “cout”, and other funcitons respectively. Since in some case, we would use both of them in our program and they won’t work as what we want without being controlled, C++ use synchronize to manage them. However, this rule will significantly slow down our program, especially when inputing and outputing large scale of data only in C or C++ format. Thus, we can close the synchronize by using codes upward. AlgorithmsWhen solving problems on LeetCode, I found some problems are really interesting, some solutions are so amazing, and some methods are valuable to learn and remember. I will post individual blogs for them and here is the index of them. 146 LRU Cache 159 Longest Substring with At Most Two Distinct Characters 200 Number of Islands problem 332 problem 380 843 Guess the word 982 Triples with Bitwise AND Equal To Zero 984 String Without AAA or BBB","categories":[{"name":"LeetCode","slug":"LeetCode","permalink":"https://lrscy.github.io/categories/LeetCode/"}],"tags":[{"name":"LeetCode","slug":"LeetCode","permalink":"https://lrscy.github.io/tags/LeetCode/"},{"name":"Summary","slug":"Summary","permalink":"https://lrscy.github.io/tags/Summary/"}]},{"title":"DeepLearning.ai Note - Neural Network and Deep Learning","slug":"DeepLearningNotes-NNandDL","date":"2018-10-22T16:32:25.000Z","updated":"2020-07-09T08:11:49.685Z","comments":true,"path":"2018/10/22/DeepLearningNotes-NNandDL/","link":"","permalink":"https://lrscy.github.io/2018/10/22/DeepLearningNotes-NNandDL/","excerpt":"","text":"This is a note of the first course of the “Deep Learning Specialization” at Coursera. The course is taught by Andrew Ng. Almost all materials in this note come from courses’ videos. The note combines knowledge from course and some of my understanding of these konwledge. I’ve reorganized the structure of the whole course according to my understanding. Thus, it doesn’t strictly follow the order of videos. In this note, I will keep all functions and equations vectorized (without for loop) as far as possible. If you want to read the notes which strictly follows the course, here are some recommendations: mbadry1’s notes on Github ppant’s notes on Github Some parts of this note are inspired from Tess Ferrandez. Brief Intro to Deep LearningTo begin with, let’s focus on some basic concepts to gain some intuition of deep learning. Stuctures of Deep LearningWe start with supervised learning. Here are several types of neural network (NN) in the folloing chart: INPUT: X OUTPUT: y NN TYPE Home features Price Standard NN Ad, user info Click or not Standard NN Image Objects Convolutional NN (CNN) Audio Text Transcription Recurrent NN (RNN) English Chineses Recurrent NN (RNN) Image, Radar info Position of other cars Custom NN Here are some pictures of Standard NN, Convolutional NN, Recurrent NN: Standard NN Convolutional NN Recurrent NN DataNeural Nework can deal with both stuctured data and unstructured data. The following will give you an intuition of both kinds of data. Structured and Unstructured Data AdvantagesHere are some conclusions of why deep learning is advanced comparing to traditional machine learning. Firstly, deep learning models performs better when dealing with big data. Here is a comparation of deep learning models and classic machine learing models: Comparation between deep learning and machine learning Secondly, thanks to the booming development of hardware and advanced algorithm, computing is much faster than before. Thus, we can implement our idea and know whether it works or not in short time. As a result, we can run the following circle much faster than we image. Iteration process Basic Symbols of the CourseThese basic symbols will be used through out the whole specialization. Standard notations Standard representation Moreover, in this course, each input x will be stacked into columns and form the input matrix X. Input X Output y Neural NetworkReviewing the whole course, there are several common concepts between logistic regression and neural network (including both shallow and deep neural network). Thus, I draw conclusions on each concept and then apply them to both logistic regression and neural network. Logistic Regression and Neural NetworkFirst of all, here are pictures of logistic regression and neural network. Logistic Regression Neural Network As we can see, logistic regression is also a kind of neural network, which has input layer and output layer and does not have hidden layers, so that it is also called mini neural network. In the following sections, I will write “neural network” to represent logistic regression and neural network and use pictures similar to the second one to represent neural network. Computation GraphComputation graph is one of basic concepts in deep learning. By analyzing it, we could understand the whole process of computation process of neural network. The following is the basic computation graph: In this picture, we can easily understand how $J(a,b,c)=3(a+bc)$ is computed. This process is similar to “Forward Propagation” process which I will say in next section. Moreover, in neural network, $J$ is called cost function. After computing cost function $J$, we need to feed it back to all of our parameters, such as $a$, $b$, $c$ in the picture. This process is called computing derivatives. By analyzing the comutation graph, we can easily compute all deviatives. According to chain rule, we can compute $\\frac{dJ}{da}$ by $\\frac{dJ}{dv}\\frac{dv}{da}$. So do parameter $b$ and $c$. The whole derivation process is similar to “backward propagation” process in neural network. Forward PropagationComputation on single neuron Computation on single neuron For every single neuron, the computing process is the same as the logistic regression. Logistic regression is basically the combination of linear regression and logistic function such as sigmoid. It has one input layer, x, and one output layer, a or $ \\hat{y} $. The linear regression equation is:&ensp;$ z = w^Tx+b $The sigmoid function equation is:&ensp;$ a = \\sigma( z ) $The combination euquation is:&emsp;&ensp;&nbsp;$ \\hat{y} = a = \\sigma( w^Tx + b ) $ The whole process on Neural Network Forward Propagation This is an example of neural network. Since it only has one hidden layer, it’s also called shallow neural network. The forward propagation process means that we compute the graph from left to the right in this picture. The whole process when computing the 1st layer (hidden layer) is as the following: $$\\begin{align}Z^{[1]} &amp; = W^{[1]T}X + b^{[1]} \\\\A^{[1]} &amp; = \\sigma( Z^{[1]} )\\end{align}$$ In these equations: $W^{[1]T}$ is a $4 \\times 3$ matrix. It is also written as $W^{[1]}$. Its shape is always $n^{[l]} \\times n^{[l - 1]}$. $X$ is a $3 \\times m$ matrix. Sometimes it is also called $A^{[0]}$. $b^{[1]}$ is a $4 \\times m$ matrix. Its shape is always $n^{[l]} \\times m$. $A^{[1]}$ is a $4 \\times m$ matrix. Its shape is always $n^{[l]} \\times m$. $\\sigma$ is an element-wise function. It is called activation function. For each layer, it just repeats what previous layers do until the last layer (output layer). Cost functionHere is a definition of loss function and cost function. Loss function computes a single training example. Cost function is the average of the loss function of the whole training set. In traditional machine learning, we use square root error as loss function, which is $ L = \\frac{1}{2}( \\hat{y} - y )^2 $. But in this case, we don’t use it since most problems we try to solve are not convex. Here is the loss function we use: $$L( \\hat{y}, y ) = -( y \\cdot log(\\hat{y}) + ( 1 - y ) \\cdot log( 1 - \\hat{y} ) )$$ For this loss function: if y = 1, then $ L = -y \\cdot log(\\hat{y}) $ and it will close to 0 when $ \\hat{y} $ near 1. if y = 0, then $ L = -( 1 - y ) \\cdot log( 1 - \\hat{y} ) $ and it will close to 0 when $ \\hat{y} $ near 0. Then the cost function is: $$ J( w, b ) = \\frac{1}{m}\\sum_{i=1}^{m} L( \\hat{y}, y ) $$ Backward PropagationHere, we use gradient descent as our backward propagation method. Compute Gradients Backward Propagation As we can see in the picture, it is a simplified computation graph. The neural network is on the right-top, which is almost the same as the neural network we discussing in previous section. Backward Propagation is computing derivatives from the right to the left. By following the backward process, we can get derivatives for all parameters, including $W^{[1]}$, $b^{[1]}$, $W^{[2]}$, $b^{[2]}$. Here I give a rough derivation example of computing gradients of parameter $W^{[1]}$. $$\\begin{align}\\frac{\\partial L}{\\partial Z^{[1]}} &amp; = W^{[2]T}\\frac{\\partial L}{\\partial Z^{[2]}} \\cdot {\\sigma}^{[1]\\prime}(Z^{[1]}) \\\\\\frac{dL}{dW^{[1]}} &amp; = \\frac{\\partial L}{\\partial Z^{[1]}}\\frac{dZ^{[1]}}{dW^{[1]}} \\\\ &amp; = \\frac{\\partial L}{\\partial Z^{[1]}}A^{[0]T} \\\\\\frac{dL}{db^{[1]}} &amp; = \\frac{\\partial L}{\\partial Z^{[1]}}\\frac{dZ^{[1]}}{db^{[1]}} \\\\ &amp; = \\frac{\\partial L}{\\partial Z^{[1]}}\\end{align}$$ It is similar to compute parameters in other layers. In these equations: $\\frac{dL}{dW^{[1]}}$ has the same shape as $W^{[1]}$. So do other layers. $\\frac{dL}{db^{[1]}}$ has the same shape as $b^{[1]}$. So do other layers. the $\\cdot$ in first line is an element-wise product. Update parametersAfter computing gradients, we can update our parameters quickly. For every parameters (Take layer1 as an example): $$\\begin{align}W^{[1]} &amp; = W^{[1]} - \\alpha \\frac{dL}{dW^{[1]}} \\\\b^{[1]} &amp; = b^{[1]} - \\alpha \\frac{dL}{db^{[1]}}\\end{align}$$ In above equations, $\\alpha$ is called learning rate, which we need to determine before training. Activation FunctionsIn previous sections, notation $\\sigma$ is used to represent activation function. In neural network, there are five common activation functions: Sigmoid, Tanh, ReLU, Leaky ReLU, and Exponential LU. Activation Functions In the course, Prof. Andrew Ng introduces the first four activation functions. Here are some experience on choosing those activation functions: Sigmoid: It is usually used in output layer to generate results between 0 and 1 when doing binary classification. In other case, you should not use it. Tanh: It always works better than sigmoid function since its value is between -1 and 1, so that neural network can learn more information by using it than using sigmoid function. ReLU: The most commonly used activation function is ReLU function. If you don’t want to use it, you can choose other ReLU derivatives, such as Leaky ReLU. Parameters and HyperparametersComparation of shallow and deep neural network","categories":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"https://lrscy.github.io/categories/Deep-Learning/"}],"tags":[{"name":"Coursera","slug":"Coursera","permalink":"https://lrscy.github.io/tags/Coursera/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"https://lrscy.github.io/tags/Deep-Learning/"}]},{"title":"数据清洗工具及相关使用方法","slug":"Data-clean-for-Machine-Translation-CE","date":"2018-03-20T18:50:09.000Z","updated":"2020-07-09T08:11:49.685Z","comments":true,"path":"2018/03/20/Data-clean-for-Machine-Translation-CE/","link":"","permalink":"https://lrscy.github.io/2018/03/20/Data-clean-for-Machine-Translation-CE/","excerpt":"最近跟着老师参加了CWMT评测任务，稍微整理下目前用到的数据清洗相关的工具及使用方法。本人目前操作全在Ubuntu16.04下进行。","text":"最近跟着老师参加了CWMT评测任务，稍微整理下目前用到的数据清洗相关的工具及使用方法。本人目前操作全在Ubuntu16.04下进行。 中文分词工具ansj安装ansjAnsj的Github地址 首先需要下载两个jar包ansj_seg和nlp-lang。一般来说都下载最新的jar包都不会有问题，只用下文件夹内名字最短的哪个jar包就可以了。如果需要用老版的ansj_seg的话，要去jar包中的.pom文件中看下对应nlp-lang的jar包，然后下载。 下载完了后在IDE中将jar包引入即可。 使用方法目前我使用的分词方式时精准分词，函数为ToAnalysis.parse()。其余方法请移步Ansj的wiki查看。分词后的输出是带有词性的，使用toStringWithOutNature函数可以去掉词性。 核心代码样例：12Result parse = ToAnalysis.parse( tmpString );String resultStr = parse.toStringWithOutNature( \" \" ); 英文分词工具tokenizer.perltokenizer.perl是统计机器翻译系统moses的一个小工具，可以用来对英文德文等进行分词。 安装tokenizer.perl因为该脚本并不是独立运行的，其需要moses自带的一些词库。moses整体不大，建议整体都下载下来。从moses的Github下载即可。该工具位置在[mosesdecoder dir]/scirpts/tokenizer/目录下。 使用方法建议将待分词文件放在上述文件夹下进行分词操作。输入如下命令进行分词：1$ perl tokenizer.perl -l en &lt; [untokenized file] &gt; [tokenized file]其中： -l是语言选择，这里选择en即英文。 &lt;代表输入，[untokenized file]是待分词文件 &gt;代表输出，[tokenized file]是分词后的输出文件，如果这个文件不存在则会创建同名文件。 其他用法可以输入下述命令查看：1$ perl tokenizer.perl -h 词对齐工具fast_align安装fast_align安装过程翻译自fast_align的Github 环境配置在Ubuntu系统上输入以下命令配置环境，其他系统请自行搜索相对应的安装包：1$ sudo apt install libgoogle-perftools-dev libsparsehash-dev如果系统中没有安装cmake请输入以下命令进行安装：1$ sudo apt install cmake 安装进入fast_align文件夹，顺序执行下述命令进行安装：1234$ mkdir build$ cd build$ cmake ..$ make上述命令中..表示的是上一层文件夹，即fast_align文件夹。安装完成后出现fast_align和atool两个文件即表示成功安装。 使用方法如果平行预料库源语言和目标语言文件是分开的，则需要手动将其合并成一个文件，格式为：[源语言句子] ||| [目标语言句子]。注意，在”|||”前后是有空格分开的。 样例Python程序：123456789101112131415161718#!/bin/python# -*- encoding: utf-8 -*-import sysdef merge( src, trg, out ): with open( src ) as fs, with open( trg ) as ft, with open( out ) as fo: for lines, linef in zip( fs, ft ): fo.write( lines.strip() + \" ||| \" + linef.strip() )def main(): if len( sys.argv ) != 4: print \"USAGE: ./python merge.py [src file] [trg file] [out file]\" else: merge( sys.argv[1], sys.argv[2], sys.argv[3] )if __name__ == \"__main__\": main() 在build文件夹下执行下述命令进行正向词对齐：1$ ./fast_align -i [filename] -d -o -v &gt; [align_forward_filename]然后在build文件夹下执行下述命令进行反向词对齐：1$ ./fast_align -i [filename] -d -o -v -r &gt; [align_reverse_filename]最后在build文件夹下执行下述命令进行双向词对齐获得最终词对齐文件：1$ ./atool -i [align_forward_filename] -j [align_reverse_filename] -c grow-diag-final-and &gt; [align_file_name]生成文件格式为i-j的格式，i代表-i文件中该行第i个词，j代表-j文件中该行第j个词。得到最终结果后，中间结果可以删除掉。 注意fast_align本身线程不安全，尽量不要一次执行多个fast_align程序。","categories":[{"name":"NLP","slug":"NLP","permalink":"https://lrscy.github.io/categories/NLP/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"https://lrscy.github.io/tags/NLP/"},{"name":"NMT","slug":"NMT","permalink":"https://lrscy.github.io/tags/NMT/"}]},{"title":"Cocos2d-x v3.16 Y轴翻转","slug":"Cocos2dx-3-16-Y-axis-Flip","date":"2018-02-12T01:42:15.000Z","updated":"2020-07-09T08:11:49.681Z","comments":true,"path":"2018/02/11/Cocos2dx-3-16-Y-axis-Flip/","link":"","permalink":"https://lrscy.github.io/2018/02/11/Cocos2dx-3-16-Y-axis-Flip/","excerpt":"","text":"Cocos2d-x的设计中，setPosition等的原点是在屏幕左下角。例如： 但是当把触摸点坐标转换到屏幕坐标点时发现其原点在左上角。例如： 这样设计的好处是自然的支持纵向的“自然滚动”（即向下划的时候上面的内容自然出现，反之亦然）。然而当不需要这个功能的时候，会在坐标变换上小折腾下。 由于在处理多点触碰和缩放时候这个坐标转换耗费了不少的时间，为了以后懒得在这个上面再费时间和脑力（就是懒emmmm），归纳一点：“旧纵坐标减新纵坐标”。今后再有类似问题就来看着这条改下就好了。","categories":[{"name":"Cocos2d-x","slug":"Cocos2d-x","permalink":"https://lrscy.github.io/categories/Cocos2d-x/"}],"tags":[{"name":"Cocos2d-x","slug":"Cocos2d-x","permalink":"https://lrscy.github.io/tags/Cocos2d-x/"},{"name":"Axis","slug":"Axis","permalink":"https://lrscy.github.io/tags/Axis/"}]},{"title":"Cocos2dx-3.16-Android-Multi-Touch","slug":"Cocos2dx-3-16-Android-Multi-Touch","date":"2018-02-12T01:40:58.000Z","updated":"2020-07-09T08:11:49.681Z","comments":true,"path":"2018/02/11/Cocos2dx-3-16-Android-Multi-Touch/","link":"","permalink":"https://lrscy.github.io/2018/02/11/Cocos2dx-3-16-Android-Multi-Touch/","excerpt":"网上有关Cocos2d-x v3.x版本的多点触控的资料并不多，在尝试时发现了如下几个坑。跳出坑最好的办法就是去读官方给出的Testapp的源码，这样能看快的了解到官方时如何使用各种功能的，模仿和学起来都很快且很到位。如果更有能力的去多看看API和源码也是很好的学习途径。","text":"网上有关Cocos2d-x v3.x版本的多点触控的资料并不多，在尝试时发现了如下几个坑。跳出坑最好的办法就是去读官方给出的Testapp的源码，这样能看快的了解到官方时如何使用各种功能的，模仿和学起来都很快且很到位。如果更有能力的去多看看API和源码也是很好的学习途径。 onTouchesBegan/Moved/Ended/Cancelled函数的参数网上众多教程基本都是基于2.x版本，所用函数名及参数有许多为cc开头，且触摸回调函数的参数基本为：12void MoreTouches::ccTouchesMoved( cocos2d::CCSet *pTouches, cocos2d::CCEvent *pEvent );在当前3.16版本中，其改为了12void onTouchesBegan( const std::vector&lt;cocos2d::Touch *&gt; &amp;touches, cocos2d::Event *event );不再使用set作为触摸点的存储结构，而是采用vector。 onTouchesBegan/onTouchesEnded传入参数touches的问题先说onTouchesBegan函数的传入参数。onTouchesEnded和onTouchesBegan的逻辑几乎是一样的。 开始参考网上多点触控的代码进行实验，但是在onTouchesBegan回调函数上永远出问题，后来发现onTouchesBegan回调函数的touches参数永远只存了一个变量（那你用vector存什么呀啊喂！）。在进行了众多测试以及上网寻找资料（不小心还挖了个坟emmmm）后决定仔细研究下官方的Testapp源码。 原先模仿网上函数写法的代码：1234void Hello::onTouchesBegan( const std::vector&lt;cocos2d::Touch *&gt; &amp;touches, cocos2d::Event *event ) &#123; if( touches.size() &gt;= 2 ) &#123; ... &#125; else &#123; ... &#125;这段代码的touches.size()的值永远是1。官方给出的MultiTouch的源代码部分如下：123456static Map&lt;int, TouchPoint *&gt; s_map;void MultiTouchTest::onTouchesBegan( const std::vector&lt;Touch *&gt; &amp;touches, Event *event ) &#123; for ( auto &amp;item: touches ) &#123; ... &#125;&#125;最开始看代码的时候还一脸欢喜，这官方给的例子不是明显的在说touches里面会存多变量的么。直到我用log打出来touches.size()后才“惊喜”的发现这值也是1（那你用vector干嘛呢遍历啥呢啊喂！）。 目前反推onTouchesBegan的设计逻辑是说一个指头的触摸激活一次onTouchesBegan，每个触摸的初始化单独做一次。这样的好处在于当多指相差时间很长才都按到屏幕上时，也能可以通过设计逻辑很容易的将其识别为多指操作，而不是多次单指操作。就算真的物理上是同时按住的时候也可能会给序列化处理成先后两次触摸。唯独麻烦的一点就是程序逻辑设计上要费点心思了。 目前本人的方法是将所有onTouchesBegan读取到的触摸都push_back到一个vector中，在onTouchesMoved中分类处理，最终每触发一次onTouchesEnded时就pop_back掉一次。至于为什么没用stack结构，因为处理onTouchesMoved处理时候的读取用vector方便些。","categories":[{"name":"Cocos2d-x","slug":"Cocos2d-x","permalink":"https://lrscy.github.io/categories/Cocos2d-x/"}],"tags":[{"name":"Cocos2d-x","slug":"Cocos2d-x","permalink":"https://lrscy.github.io/tags/Cocos2d-x/"},{"name":"Android","slug":"Android","permalink":"https://lrscy.github.io/tags/Android/"},{"name":"Multi Touch","slug":"Multi-Touch","permalink":"https://lrscy.github.io/tags/Multi-Touch/"}]},{"title":"Cocos2d-x v3.16 Android Studio添加新类","slug":"Cocos2dx-3-16-Android-Studio-Add-New-Class","date":"2018-02-04T01:29:01.000Z","updated":"2020-07-09T08:11:49.681Z","comments":true,"path":"2018/02/03/Cocos2dx-3-16-Android-Studio-Add-New-Class/","link":"","permalink":"https://lrscy.github.io/2018/02/03/Cocos2dx-3-16-Android-Studio-Add-New-Class/","excerpt":"在使用Android Studio编辑Cocos2d-x项目时，如果想添加一个Class进去，除了建立相对应的.h和.cpp以外，还需要让编译配置文件知道这个文件属于该项目。然而Android Studio 3.x版本自动同步时并不能将新类中的.cpp问家加入编译配置文件中。","text":"在使用Android Studio编辑Cocos2d-x项目时，如果想添加一个Class进去，除了建立相对应的.h和.cpp以外，还需要让编译配置文件知道这个文件属于该项目。然而Android Studio 3.x版本自动同步时并不能将新类中的.cpp问家加入编译配置文件中。 后来发现在左侧External Build Files中，有个叫做Android.mk的文件，需要在其中的LOCAL_SRC_FILES变量中加入新的.cpp文件。即：1234LOCAL_SRC_FILES := $(LOCAL_PATH)/hellocpp/main.cpp \\ $(LOCAL_PATH)/../../../Classes/AppDelegate.cpp \\ $(LOCAL_PATH)/../../../Classes/HelloWorldScene.cpp \\ $(LOCAL_PATH)/../../../Classes/Hello.cpp想必大家都读得懂怎么向这个变量中加入新的文件，其中Hello.cpp就是我添加的新.cpp文件。","categories":[{"name":"Cocos2d-x","slug":"Cocos2d-x","permalink":"https://lrscy.github.io/categories/Cocos2d-x/"}],"tags":[{"name":"Cocos2d-x","slug":"Cocos2d-x","permalink":"https://lrscy.github.io/tags/Cocos2d-x/"},{"name":"Android Studio","slug":"Android-Studio","permalink":"https://lrscy.github.io/tags/Android-Studio/"}]},{"title":"Ubuntu 16.04下TensorFlow安装","slug":"Ubuntu-Tensorflow-config","date":"2018-02-02T18:29:40.000Z","updated":"2020-07-09T08:11:49.693Z","comments":true,"path":"2018/02/02/Ubuntu-Tensorflow-config/","link":"","permalink":"https://lrscy.github.io/2018/02/02/Ubuntu-Tensorflow-config/","excerpt":"前言TensorFlow安装环境为Ubuntu 16.04.3 LTS，GPU为GT 750M。 假设目前已经安装好了CUDA8，如果没有安装请依照「Ubuntu 16.04下CUDA Tookit 8安装」进行安装。 如果有一定英语能力的同学最好请移步官网进行下载安装，尽管可能需要下科学上网。","text":"前言TensorFlow安装环境为Ubuntu 16.04.3 LTS，GPU为GT 750M。 假设目前已经安装好了CUDA8，如果没有安装请依照「Ubuntu 16.04下CUDA Tookit 8安装」进行安装。 如果有一定英语能力的同学最好请移步官网进行下载安装，尽管可能需要下科学上网。 基础环境搭建本文采用Virtualenv环境进行搭建，这样能将TensorFlow运行于一个分离开的Python环境下，免得今后出错时对整个系统产生影响。 CUPTI环境搭建CUPTI库能够提高可以提高CUDA的性能，官方要求安装。 如果CUDA Tookit &gt;= 8.0，输入如下命令安装：1$ sudo apt install cuda-command-line-tools并且在CUDA安装时所设定的LD_LIBRARY_PATH添加路径至如下状态：1export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/extras/CUPTI/lib64如果CUDA Tookit &lt;= 7.5或者上述命令无法执行，请输入如下命令安装CUPTI库：1$ sudo apt-get install libcupti-dev pip环境搭建下面两行中选择一行执行即可，依据个人Python版本选择：12$ sudo apt install python-pip python-dev python-virtualenv # for Python 2.7$ sudo apt install python3-pip python3-dev python-virtualenv # for Python 3.x本人选择的是Python2.7，因为Ubuntu 16.04.3默认Python为Python 2.7，改默认会影响ibus-pinyin（输入法）的更新和运行。 Virtualenv环境搭建下面两行中选择一行执行即可，依据个人Python版本选择：12$ virtualenv --system-site-packages targetDirectory # for Python 2.7$ virtualenv --system-site-packages -p python3 targetDirectory # for Python 3.x其中targetDirectory是隔离环境的根目录，需要自行设定。个人设定是~/tensorflow，下文暂且按照这个目录进行介绍。 至此，基础观景搭建完成。 TensorFlow安装进入Virtualenv环境输入如下命令进入Virtualenv环境：12$ source ~/tensorflow/bin/activate # bash, sh, ksh, or zsh$ source ~/tensorflow/bin/activate.csh # csh or tcsh请依据自身shell环境选择，一般Ubuntu原生使用的是sh。 这里可以在~/.bashrc中输入如下命令使得命令简化：12alias tensorflow=\"source ~/tensorflow/bin/activate\" # bash, sh, ksh, or zshalias tensorflow=\"source ~/tensorflow/bin/activate.csh\" # csh or tcsh请依据自身的shell环境选择第一条还是第二条。其中tensorflow可以自行拟定。 当编辑完后在命令行中输入如下命令使其立即生效：1$ source ~/.bashrc以后在命令行中输入tensorflow即可调出该Virtualenv环境。 当前环境应该如下：1(tensorflow) xxx@xxx:path$ tensorflow即Virtualenv文件夹名称，下文按照tensorflow描述。 xxx@xxx:path与未进入Virtualenv环境时无差别。下文中将简化描述为(tensorflow) $ 更新pip为了确保pip version &gt;= 8.1，输入如下命令更新pip：12(tensorflow) $ pip install --upgrade pip # for Python 2.7(tensorflow) $ pip3 install --upgrade pip # for Python 3.x 安装TensorFlow如果系统中的CUDA Tookit和cuDNN都是最新版，请从以下四条命令中选择一个来安装TensorFlow：1234(tensorflow) $ pip install --upgrade tensorflow # for Python 2.7(tensorflow) $ pip3 install --upgrade tensorflow # for Python 3.x(tensorflow) $ pip install --upgrade tensorflow-gpu # for Python 2.7 and GPU(tensorflow) $ pip3 install --upgrade tensorflow-gpu # for Python 3.x and GPU前两条是仅使用CPU的版本，后两条的版本能够使用GPU。 如果系统中的CUDA Tookit和cuDNN并非最新版，按照上述安装很可能会出现问题导致重装。因此安装前请先完成如下几步： 请先确定系统中的cuDNN和CUDA Tookit时想匹配的，具体版本匹配详见官网。 请在TensorFlow的Github Release中寻找符合自己CUDA Tookit和cuDNN的版本。 依照选好的版本拼凑网址。 拼凑网址如下，请依据CPU以及GPU从两条中选择一条：1234# for CPU onlyhttps:&#x2F;&#x2F;storage.googleapis.com&#x2F;tensorflow&#x2F;linux&#x2F;cpu&#x2F;tensorflow-x.x.x-cpxx-none-linux_x86_64.whl# for GPU supporthttps:&#x2F;&#x2F;storage.googleapis.com&#x2F;tensorflow&#x2F;linux&#x2F;gpu&#x2F;tensorflow_gpu-x.x.x-cpxx-none-linux_x86_64.whl其中： x.x.x是从release中选择出的需要的版本。 cpxx是本机python的版本，例如：2.7.xx为27，3.5.xx的版本为35，以此类推。 storage.googleapis.com可能需要科学上网才能访问。 然后进行Tensorflow的安装，输入如下命令：12(tensorflow) $ pip install yourhttpsite # for Python 2.7(tensorflow) $ pip3 install yourhttpsite # for Python 3.xyourhttpsite就是拼凑出来的网址。 如果pip提示无法从代理处下载文件，则可以先将文件从网址下载下来并存在~（即家目录）或者自定义目录下，然后通过如下命令安装：12(tensorflow) $ pip install filename # for Python 2.7(tensorflow) $ pip3 install filename # for Python 3.xfilename即下载的.whl文件（可能需要输入完整路径）。 TensorFlow的使用启动TensorFlow环境请参照进入Virtualenv环境步骤进行。 测试TensorFlow环境先进入TensorFlow环境，然后输入如下代码进行测试：1234import tensorflow as tfhello = tf.constant('Hello, TensorFlow!')sess = tf.Session()print(sess.run(hello))输出Hello, TensorFlow!且无报错即可。如果报错请参考官网或者上网寻找解决方案。 退出TensorFlow环境在环境中输入deactivate即可，即：1(tensorflow) $ deactivate TensorFlow的卸载卸载时直接将其所在的Virtualenv文件夹删除即可，命令如下：1$ rm -rf targetDirectorytargetDirectory即Virtualenv环境文件夹。","categories":[{"name":"Config","slug":"Config","permalink":"https://lrscy.github.io/categories/Config/"}],"tags":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://lrscy.github.io/tags/Ubuntu/"},{"name":"Tensorflow","slug":"Tensorflow","permalink":"https://lrscy.github.io/tags/Tensorflow/"}]},{"title":"Ubuntu 16.04下CUDA Tookit 8安装","slug":"Ubuntu-CUDA-Tookit-8-Install","date":"2018-02-01T16:39:17.000Z","updated":"2020-07-09T08:11:49.693Z","comments":true,"path":"2018/02/01/Ubuntu-CUDA-Tookit-8-Install/","link":"","permalink":"https://lrscy.github.io/2018/02/01/Ubuntu-CUDA-Tookit-8-Install/","excerpt":"前言前一阵重做系统把之前的安装教程的博客给弄丢了，现在重写一份做记录好了。目前老环境还是要CUDA Tookit 8（以下简称CUDA8），因此目前先不安装CUDA最新的CUDA9了。 本教程是在Ubuntu 16.04.3 LTS上进行的，其他版本的Ubuntu系统请自行实验，目前在Ubuntu 17.10系统上实验成功。 当前假设Ubuntu系统已经安装完成，目前直接进行CUDA8的安装。","text":"前言前一阵重做系统把之前的安装教程的博客给弄丢了，现在重写一份做记录好了。目前老环境还是要CUDA Tookit 8（以下简称CUDA8），因此目前先不安装CUDA最新的CUDA9了。 本教程是在Ubuntu 16.04.3 LTS上进行的，其他版本的Ubuntu系统请自行实验，目前在Ubuntu 17.10系统上实验成功。 当前假设Ubuntu系统已经安装完成，目前直接进行CUDA8的安装。 基础准备基础环境安装安装CUDA8需要基础的编译环境，需要检测下系统上是否安装了gcc或g++。命令如下：12$ gcc -v$ g++ -v如果任意一个出现版本信息就代表安装过了。如果都没有出现版本信息，则请采用如下命令安装：1$ sudo apt install build-essential安装完成即可，网上有说CUDA8不支持g++ 5.0以上的版本，目前本人没有遇上这个问题。 附加环境安装有些人在上述基础环境下安装完CUDA8后会出现Missing recommended library: libGLU.so的提示。如果不确定当前环境想提前避免这个问题，请安装如下包：1sudo apt install libglu1-mesa libxi-dev libxmu-dev libglu1-mesa-dev至此，基础环境都安装完了。 NVIDIA驱动安装个人强烈建议先装NVIDIA驱动，因为CUDA8自带的驱动实在是容易出问题。驱动安装详见「Ubuntu系统NVIDIA显卡驱动安装」。 CUDA8安装配置CUDA8下载先从官网下载CUDA的驱动。目前CUDA的最新版本是CUDA9，要下老版本的话请采用如下官方网址，下载Base Installer和Patch 2。 个人建议先将文件都下载到家目录(~)底下，因为如果系统是中文环境的话，后续安装可能会出现文件夹名乱码的情况。安装完成后可以移动到自己想移动的位置。 CUDA8安装是否进入tty1环境看个人，本人之前各种重装驱动给吓怕了，不确定图形界面是否影响到了，因此直接进入了tty1环境进行CUDA8的安装。关闭图形界面的步骤如下： 首先按住Ctrl+Alt+F1进入tty1 输入用户名和密码 执行sudo service lightdm stop命令关闭图形界面。 然后在安装文件所在目录下执行如下命令安装：12$ sudo chmod 755 (CUDA Install File)$ sudo ./(CUDA Install File)其中CUDA Install File是个人CUDA安装文件的名字（包括文件后缀）。 安装关键过程如下（先后顺序记不清了）： 安装过程中先阅读完一大串协议，按住d往下（这样跳得很快），直到最后。然后输入accept。 会询问是否安装NVIDIA DRIVER，输入n。 询问是否安装OPENGL时输入n。（记得有这个问题的，这里是个大坑！！） 其他默认选择y或者空着（就是直接按回车）就行。 到最后可能会出现INCOMPLETE INSTALL，这里不用管。这是因为你没装它的驱动而已。如果出现Missing recommended library: libGLU.so的提示，请参考附加环境安装 等一切都安装好后重启即可。 补丁安装是否进入tty1环境依旧看个人，具体参考上一节开头。 安装时执行如下命令安装：12$ sudo chmod 755 (CUDA Patch File)$ sudo ./(CUDA Patch File)其中CUDA Patch File是之前下载的CUDA8的补丁文件Patch 2。安装过程和前一节类似，基本上直接一路默认就行。 等一切安装完了后重启即可。 环境设置个人习惯在/etc/profile.d下设置环境变量。先进入该文件夹，然后执行如下命令：12$ sudo touch cuda.sh$ sudo vim cuda.sh这里vim可以换成gedit或者其他熟悉的编辑器。如果不熟悉vim的同学无意间进去了，输入:q退出vim。 在编辑器中输入如下文字：123export CUDA_HOME&#x3D;&#x2F;usr&#x2F;local&#x2F;cuda-8.0export PATH&#x3D;$PATH:&#x2F;usr&#x2F;local&#x2F;cuda-8.0&#x2F;binexport LD_LIBRARY_PATH&#x3D;$LD_LIBRARY_PATH:&#x2F;usr&#x2F;local&#x2F;cuda-8.0&#x2F;lib64然后保存重启即可。 cuDNN安装cuDNN是个GPU加速库，能为深度学习网络的计算加速。在官网下载。下载前可能需要先注册NVIDIA DEVELOPER。注册过程很简单，注册完成后选择支持CUDA8.0的cuDNN下载即可。 在下载目录解压后进入cuda文件夹，这里会见到include和lib64两个文件夹。这里建议在命令行下执行，因为会用到sudo进行暂时的root权限申请。输入如下命令进行安装：12$ sudo cp include/cudnn.h /usr/local/cuda/include$ sudo cp lib64/libcudnn* /usr/local/cuda/lib64然后进入/usr/local/cuda/lib64文件夹中，执行如下命令：1234$ sudo rm libcudnn.so libcudnn.so.6$ sudo ln -s libcudnn.so.6.0.20 libcudnn.so.6$ sudo ln -s libcudnn.so.6 libcudnn.so$ sudo ldconfig这里.6和.6.0.20是下载的cuDNN的版本号，请依据个人下载的实际版本进行修改。 至此，cuDNN已经安装完成。 结尾至此，整个CUDA Tookit 8的所有安装过程就完成了。","categories":[{"name":"Config","slug":"Config","permalink":"https://lrscy.github.io/categories/Config/"}],"tags":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://lrscy.github.io/tags/Ubuntu/"},{"name":"CUDA","slug":"CUDA","permalink":"https://lrscy.github.io/tags/CUDA/"}]},{"title":"Cocos2d-x v3.16 屏幕显示偏移","slug":"Cocos2dx-3-16-Display-Offset","date":"2018-01-27T01:28:36.000Z","updated":"2020-07-09T08:11:49.681Z","comments":true,"path":"2018/01/26/Cocos2dx-3-16-Display-Offset/","link":"","permalink":"https://lrscy.github.io/2018/01/26/Cocos2dx-3-16-Display-Offset/","excerpt":"","text":"在v3.16前，屏幕上显示Sprites, Nodes都是直接使用Director类获取可视区大小并直接在上面定位Nodes的，例如：123auto size = Director::getInstance()-&gt;getVisibleSize();auto sprite = Sprite::create( \"HelloWorld.png\" );sprite.setPosition( size.width / 2, size.height / 2 );目前不知道时从哪个版本开始的，直接使用上述代码会导致sprite下偏移(沿y轴)一定量但无左右偏移(沿x轴)。查找资料未果后看Helloworld样例，发现其用到了Director类中另一个变量纠正这个问题：1Vec2 origin = Director::getInstance()-&gt;getVisibleOrigin();这里origin是真正的屏幕左下角的位置，所有屏幕上的显示的图像都要依据这个坐标进行调整。也就是说上述代码要改成：1234auto size = Director::getInstance()-&gt;getVisibleSize();Vec2 origin = Director::getInstance()-&gt;getVisibleOrigin();auto sprite = Sprite::create( \"HelloWorld.png\" );sprite.setPosition( origin.x + size.width / 2, origin.y + size.height / 2 );这时，屏幕偏移问题解决了，sprite也在屏幕的真正的正中央。","categories":[{"name":"Cocos2d-x","slug":"Cocos2d-x","permalink":"https://lrscy.github.io/categories/Cocos2d-x/"}],"tags":[{"name":"Cocos2d-x","slug":"Cocos2d-x","permalink":"https://lrscy.github.io/tags/Cocos2d-x/"},{"name":"Display Offset","slug":"Display-Offset","permalink":"https://lrscy.github.io/tags/Display-Offset/"}]},{"title":"Cocos2d-x v3.16踩过的坑","slug":"Cocos2dx-3-16-Problems","date":"2018-01-26T21:35:23.000Z","updated":"2020-07-09T08:11:49.681Z","comments":true,"path":"2018/01/26/Cocos2dx-3-16-Problems/","link":"","permalink":"https://lrscy.github.io/2018/01/26/Cocos2dx-3-16-Problems/","excerpt":"","text":"前言最近刚碰关于Cocos2d-x的知识，也上网查过很多资料，但是很少有讲最新v3.16的博客。因此在此记录下使用v3.16时候遇上的坑。且会不定期更新该篇博客。 UPDATE 2018.02.11:之前是写在一起的，但是看起来太乱，就拆分到各自单独篇章中去了，这里只做个目录。 屏幕显示偏移 Android Studio添加新类 多点触控 Y轴翻转 To be continue…","categories":[{"name":"Cocos2d-x","slug":"Cocos2d-x","permalink":"https://lrscy.github.io/categories/Cocos2d-x/"}],"tags":[{"name":"Cocos2d-x","slug":"Cocos2d-x","permalink":"https://lrscy.github.io/tags/Cocos2d-x/"},{"name":"Android Studio","slug":"Android-Studio","permalink":"https://lrscy.github.io/tags/Android-Studio/"}]},{"title":"在Github上备份Hexo博客","slug":"Hexo-Github-Backup","date":"2018-01-26T17:57:56.000Z","updated":"2020-07-09T08:11:49.693Z","comments":true,"path":"2018/01/26/Hexo-Github-Backup/","link":"","permalink":"https://lrscy.github.io/2018/01/26/Hexo-Github-Backup/","excerpt":"","text":"前言由于之前忘记备份Hexo博客的markdown文件，在重做系统时候还忘记备份博客了，导致现在不得不重新从网页上扒下来之前的文章重新写一遍，十分耗费精力。因此在网上找了下如何备份Hexo博客，在此记录下。 目前假设Git和Github环境已经配置好了，如果没有配置好详见「Ubuntu16.04下Github配置」。Git相关操作请参考廖雪峰的Git教程。 备份博客目前假设本地Hexo博客已经初始化，如果没有配置好Hexo博客详见「Ubuntu16.04下从零起步搭建配置github.io博客————Hexo」。 创建新分支在Github.io上建立博客时已经开了一个新仓库了，如果再开另一个仓库存放源代码有点浪费，因此采用建立新分支的方法备份博客。 虽然理论上什么时候创建新分支来备份都可以，但是还是建议在建立博客的时候就创建备份分支。（然而我中途才想起来-.-） 不过在建立新分支前请确保仓库内已有master分支（Hexo本地建站后第一次上传时会自动生成），否则后期再添加master分支比较麻烦（请自行搜索git命令）。 本地Git建立新分支命令如下：1$ git checkout -b BRANCHNAMEBRANCHNAME是自定义的新分支的名字，建议起为hexo。 建立.gitignore建立.gitignore文件将不需要备份的文件屏蔽。个人的.gitignore文件如下：1234567.DS_StoreThumbs.dbdb.json*.lognode_modules&#x2F;public&#x2F;.deploy*&#x2F; 在Github上备份通过如下命令将本地文件备份到Github上。 假设目前在hexo博客的根目录下。123$ git add .$ git commit -m \"Backup\"$ git push origin hexo这样就备份完博客了且在Github上能看到两个分支(master和hexo)。 设置默认分支在Github上你的github.io仓库中设置默认分支为hexo。这样有助于之后恢复博客。master分支时默认的博客静态页面分支，在之后恢复博客的时候并不需要。 个人备份习惯个人而言习惯于先备份文件再生成博客。即先执行git add .,git commit -m &quot;Backup&quot;,git push origin hexo将博客备份完成，然后执行hexo g -d发布博客。 恢复博客目前假设本地Hexo博客基础环境已经搭好，如果没有配置好Hexo博客基础环境详见「Ubuntu16.04下从零起步搭建配置github.io博客————Hexo」。 克隆项目到本地输入下列命令克隆博客必须文件(hexo分支)：1$ git clone https://github.com/yourgithubname/yourgithubname.github.io 恢复博客在克隆的那个文件夹下输入如下命令恢复博客：123$ npm install hexo-cli$ npm install$ npm install hexo-deployer-git在此不需要执行hexo init这条指令，因为不是从零搭建起新博客。 完成喵～","categories":[{"name":"Config","slug":"Config","permalink":"https://lrscy.github.io/categories/Config/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://lrscy.github.io/tags/Hexo/"},{"name":"Github","slug":"Github","permalink":"https://lrscy.github.io/tags/Github/"}]},{"title":"Hexo中图片插入问题","slug":"Hexo-Picture-Insert","date":"2018-01-26T14:59:05.000Z","updated":"2020-07-09T08:11:49.693Z","comments":true,"path":"2018/01/26/Hexo-Picture-Insert/","link":"","permalink":"https://lrscy.github.io/2018/01/26/Hexo-Picture-Insert/","excerpt":"","text":"前言在用Hexo搭建博客时有时需要插入图片，但是原生Hexo对图片管理的支持不是很好。此篇博客记录利用插件和Hexo的配置解决这个问题。 _config.yml配置更改在Hexo根目录下，_config.yml文件管理整个Hexo的配置设置。其中要开启post_asset_folder，即：1post_asset_folder: true更改完成后，每新生成一片文章，就会在同级目录下生成一个名字相同的相对应的文件夹。图片存在该文件夹下即可。 安装插件由于原生Hexo资源文件夹在生成真正博客时地址转换有问题，需要安装插件进行修正。执行如下命令安装插件：1$ npm install https://github.com/CodeFalling/hexo-asset-image --save当安装完成后就可以在写Markdown时很容易的使用资源文件夹下的图片了。 使用教程在插入图片时只要使用如下Markdown语法即可1![](文章名字/图片名字.后缀)不知道我是不是因为在_config.yml中开启了relative_link，我采用如下方式插入图片：1![](图片名字.后缀)这点以后再探究好了喵～","categories":[{"name":"Config","slug":"Config","permalink":"https://lrscy.github.io/categories/Config/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://lrscy.github.io/tags/Hexo/"},{"name":"Markdown","slug":"Markdown","permalink":"https://lrscy.github.io/tags/Markdown/"}]},{"title":"Markdown中MathJax的用法","slug":"Markdown-MathJax-Usage","date":"2018-01-26T14:25:33.000Z","updated":"2020-07-09T08:11:49.693Z","comments":true,"path":"2018/01/26/Markdown-MathJax-Usage/","link":"","permalink":"https://lrscy.github.io/2018/01/26/Markdown-MathJax-Usage/","excerpt":"","text":"前言Markdown和MathJax在一些语法上有交集，在此记录下两者有冲突的地方，作为今后的提醒。 下划线在Markdwon中，下划线代表斜体，例如：_a_的效果既是a。在MathJax中，下划线代表下标，例如：`’的效果既是。 在Markdown解析过程中，可能会出现错误解析MathJax下划线的事情。因此在MathJax公式中要将_替换成\\_，将下划线转义成真正的下划线符号。 多行公式在MathJax中，\\\\代表换行。在Markdown中，\\\\代表将转义字符\\转义成真正的\\字符，因此写\\\\后被解析出来时只有一个\\，因此无法达成换行效果。因此在写换行时连续输入三个\\即\\\\\\即可达成换行要求。例如：12345\\begin{align}r\\_t &amp;= \\sigma(W\\_rx\\_t + Urh\\_{t-1} + b\\_r) \\\\\\u\\_t &amp;= \\sigma(W\\_ux\\_t + r\\_t \\odot (U\\_uh\\_{t-1}) + b\\_u) \\\\\\h\\_t &amp;= u\\_t \\odot h\\_{t-1} + (1-u\\_t) \\odot \\tanh(Wx\\_t + r\\_t \\odot (U\\_uh\\_{t-1}) + b)\\end{align}执行结果如下： 目前只踩到了这些坑，今后再有新坑再往后填入。","categories":[{"name":"Usage","slug":"Usage","permalink":"https://lrscy.github.io/categories/Usage/"}],"tags":[{"name":"Markdown","slug":"Markdown","permalink":"https://lrscy.github.io/tags/Markdown/"},{"name":"MathJax","slug":"MathJax","permalink":"https://lrscy.github.io/tags/MathJax/"}]},{"title":"Ubuntu 16.04下从零起步搭建配置github.io博客——Hexo","slug":"Ubuntu-Github-io-config-Hexo","date":"2017-11-10T05:00:00.000Z","updated":"2020-07-09T08:11:49.693Z","comments":true,"path":"2017/11/10/Ubuntu-Github-io-config-Hexo/","link":"","permalink":"https://lrscy.github.io/2017/11/10/Ubuntu-Github-io-config-Hexo/","excerpt":"前言本文利用Github io和Hexo搭建静态博客。主题更换等问题请到Hexo Theme里寻找并替换。 继上次用Jekyll搭建博客后，又忙了很多其他事情，接触到了Hexo。因此决定将博客从Jekyll换到Hexo。 本人是个前端小白，按照网上众多教程搭建时候依旧踩了很多坑，在此记录下来以便有相同问题的同学可以快速解决。 搭建Github io静态博客所需基础之基础的知识如下： Git Github Markdown Hexo 本文是在Ubuntu 17.04环境下配置的，如果使用其他操作系统请自行查找对应命令或者解决方案。 以下代码区域，带有$打头的表示需要在控制台（终端或称命令行）下面执行（不包括$符号）。如果出现权限不足提示请在命令最前面加上sudo再执行。 本文几乎所有命令都可以直接拷到控制台（终端或称命令行）内直接执行而不用理解其具体含义（除非特殊表明需要修改），但是强烈不建议这么做！！！ 我是基于Hexo模板搭建的博客，所以不会讲如何从零手敲出一个博客样式出来，但会比较详细的讲模板中哪里需要修改。","text":"前言本文利用Github io和Hexo搭建静态博客。主题更换等问题请到Hexo Theme里寻找并替换。 继上次用Jekyll搭建博客后，又忙了很多其他事情，接触到了Hexo。因此决定将博客从Jekyll换到Hexo。 本人是个前端小白，按照网上众多教程搭建时候依旧踩了很多坑，在此记录下来以便有相同问题的同学可以快速解决。 搭建Github io静态博客所需基础之基础的知识如下： Git Github Markdown Hexo 本文是在Ubuntu 17.04环境下配置的，如果使用其他操作系统请自行查找对应命令或者解决方案。 以下代码区域，带有$打头的表示需要在控制台（终端或称命令行）下面执行（不包括$符号）。如果出现权限不足提示请在命令最前面加上sudo再执行。 本文几乎所有命令都可以直接拷到控制台（终端或称命令行）内直接执行而不用理解其具体含义（除非特殊表明需要修改），但是强烈不建议这么做！！！ 我是基于Hexo模板搭建的博客，所以不会讲如何从零手敲出一个博客样式出来，但会比较详细的讲模板中哪里需要修改。 基础环境搭建Git/Github/SSH配置详见我的博客「Ubuntu16.04下Github配置」。 Node.js安装安装Hexo前需要安装Node.js，本人安装的是Node.js 8。 对于Ubuntu系列系统，执行以下两个命令：12$ curl -sL https://deb.nodesource.com/setup_8.x | sudo -E bash -$ sudo apt install -y nodejs如果系统中没有安装curl，执行下述命令安装：1$ sudo apt install curl安装完成后需要检查下node和npm的版本：12$ node -v$ npm -v分别显示出版本号就算安装完成了～本站搭建时node版本为v8.9.1，npm版本为5.5.1。 参考资料Node.js官网。 Hexo本地建站Hexo安装Hexo安装非常简单，上述环境搭建好后只需执行以下命令即可：1$ npm install -g hexo-cli本站搭建时Hexo的版本是3.4.0。 Hexo本地建站首先通过终端进入希望建站的文件夹内（例如~/Hexo），执行以下命令：1$ hexo init该命令要求建站文件夹是全空的文件夹。如果之前在该文件夹内建立了git等文件（夹），请先移出文件夹，建站完成后再移回来。 下述命令会在该文件夹下建立所有需要的文件。接下来安装依赖包：1$ npm install至此，Hexo本地博客已经搭建完成。对，你没看错～ 然后执行以下命令来浏览本地站点12$ hexo generate$ hexo server hexo generate是用来编译生成站点，每次对站点内容编辑后都要进行该项操作。可以简化为hexo g。 hexo server是用来启动本地站点，执行后即可在浏览器中输入localhost:4000查看。可以简化为hexo s。 参考资料Hexo 官方文档 Github io部署博客_config.yml参数设置部署配置在_config.yml文件的末尾，默认样子如下：12345# Deployment## Docs: https:&#x2F;&#x2F;hexo.io&#x2F;docs&#x2F;deployment.htmldeploy:type:repo:修改后如下：123456# Deployment## Docs: https:&#x2F;&#x2F;hexo.io&#x2F;docs&#x2F;deployment.htmldeploy:type: gitrepo: http:&#x2F;&#x2F;github.com&#x2F;xxx&#x2F;xxx.github.io.git(xxx是Github账户名称)branch: master 由于是部署到Github中，所以type是git。 repo是指Github对应仓库的SSH地址。点击该仓库页面右侧绿色download，里面的地址就是SSH地址。 branch是指上传到Github的哪个分支，如果没特殊需求选择master就可以。特殊需求请自行填写上传哪个branch。 插件安装为了部署到Github上，需要安装hexo-deployer-git插件，命令如下：1$ npm install hexo-deployer-git --save 最终部署最终部署需要输入以下两个命令：12$ hexo generate$ hexo deploy hexo generate同上。 hexo deploy将hexo部署到Github io上。可以简化为hexo d。 上传后需要等待几分钟，然后就可以在浏览器中输入xxx.github.io来欣赏了喵～ 参考资料Hexo 部署","categories":[{"name":"Config","slug":"Config","permalink":"https://lrscy.github.io/categories/Config/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://lrscy.github.io/tags/Hexo/"},{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://lrscy.github.io/tags/Ubuntu/"},{"name":"Github.io","slug":"Github-io","permalink":"https://lrscy.github.io/tags/Github-io/"}]},{"title":"Ubuntu系统NVIDIA显卡驱动安装","slug":"Ubuntu-NVIDIA-Driver-Install","date":"2017-10-10T04:00:00.000Z","updated":"2020-07-09T08:11:49.693Z","comments":true,"path":"2017/10/10/Ubuntu-NVIDIA-Driver-Install/","link":"","permalink":"https://lrscy.github.io/2017/10/10/Ubuntu-NVIDIA-Driver-Install/","excerpt":"前言本文是在Ubuntu 17.04系统上安装NVIDIA驱动。 在Ubuntu 16.04 LTS上安装驱动时是从NVIDIA 官网上下载.run文件然后按照提示安装，但是在Ubuntu 17.04上并不成功。因此寻找到了一种更为简便的方法，记录在此。 如果要在本机安装NVIDIA Driver的话请先用手机等设备照下关键步骤然后再执行。","text":"前言本文是在Ubuntu 17.04系统上安装NVIDIA驱动。 在Ubuntu 16.04 LTS上安装驱动时是从NVIDIA 官网上下载.run文件然后按照提示安装，但是在Ubuntu 17.04上并不成功。因此寻找到了一种更为简便的方法，记录在此。 如果要在本机安装NVIDIA Driver的话请先用手机等设备照下关键步骤然后再执行。 安装准备查找合适的驱动在NVIDIA 官网上寻找合适的驱动，并记住其版本号。例如，384.98的版本号是384。 关闭图形界面首先按住Ctrl+Alt+F1进入tty1模式，然后输入如下代码关闭图形界面（X Server）：1$ sudo stop lightdmlightdm指的是图形界面服务 卸载之前的NVIDIA显卡驱动如果之前有尝试过其他驱动，则需要将其卸载步骤如下：1$ sudo apt remove nvidia-*如果采用的是.run文件安装的系统请采用如下命令卸载：1$ sudo ./NVIDIA-*.run --uninstall这里NVIDIA-*.run是下载的.run文件全名。 安装NVIDIA显卡驱动因为无法通过.run方式安装驱动，因此采用从第三方源的方式安装驱动。 添加第三方源这次采用的是mamarley源，添加该源的命令如下：1$ sudo add-apt-repository ppa:mamarley/nvidia然后进行更新源1$ sudo apt update 安装NVIDIA显卡驱动在添加好第三方源后采用apt方式安装，命令如下：1$ sudo apt install nvidia-384本文安装的是384版本，请按照个人不同的需求输入合适的驱动版本号。 至此，驱动已经安装完成，但是请不要立即启动lightdm服务。请先输入以下命令重启系统：1$ reboot 检查安装结果在命令行输入如下结果：12$ nvidia-smi$ nvidia-settings输入nvidia-smi命令后输出一个表格即为正常。输入nvidia-settings出现下图即为正常。 异常处理如果安装显卡后出现循环登录不进入桌面的情况请执行“安装准备”中的“关闭图形界面”和“卸载之前的NVIDIA显卡驱动”。 卸载驱动卸载驱动请按照“卸载之前NVIDIA驱动”所述内容进行卸载并重启。","categories":[{"name":"Config","slug":"Config","permalink":"https://lrscy.github.io/categories/Config/"}],"tags":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://lrscy.github.io/tags/Ubuntu/"},{"name":"NVIDIA","slug":"NVIDIA","permalink":"https://lrscy.github.io/tags/NVIDIA/"}]},{"title":"Ubuntu 16.04下(多)SSH配置","slug":"Ubuntu-SSH-config","date":"2017-05-02T04:00:00.000Z","updated":"2020-07-09T08:11:49.693Z","comments":true,"path":"2017/05/02/Ubuntu-SSH-config/","link":"","permalink":"https://lrscy.github.io/2017/05/02/Ubuntu-SSH-config/","excerpt":"前沿本文记录了我配置同机多SSH时候的全过程。本文是在Ubuntu 16.04 LTS下配置的，如果使用其他操作系统请自行查找对应命令或解决方案。以下代码区域，带有$打头的表示需要在控制台（终端或称命令行）下面执行（不包括$符号）","text":"前沿本文记录了我配置同机多SSH时候的全过程。本文是在Ubuntu 16.04 LTS下配置的，如果使用其他操作系统请自行查找对应命令或解决方案。以下代码区域，带有$打头的表示需要在控制台（终端或称命令行）下面执行（不包括$符号） SSH安装Ubuntu 16.04 LTS自带openssh客户端，但是不带openssh服务器端。如果需要ssh localhost连接本地，那么需要安装openssh-server，命令如下：1$ sudo apt install openssh-server如果你的机器上没有装客户端的话需要执行以下命令安装：1$ sudo apt install openssh-client至此，基础环境已经安装完了。 单SSH配置此节操作只适用于只需要免密码登录一个远程服务器的同学，如果你需要同时SSH远程登录多个服务器，那么请直接跳至下一节。 对于单SSH配置，你只需要输入以下命令即可，此时为默认状态，你的个人信息不会被加入进去：1$ ssh-keygen -t rsa如果你需要将个人信息（如邮箱）加入进去（比如远程访问Github时），那么请执行如下命令：1$ ssh-keygen -t rsa -C \"youraddress@youremail.com\"这里有几点需要注意的： -C中的C是大写字母 youraddress@youremail.com是你Github的注册邮箱 对于上述两条命令，如果你远程登录时候需要设置密码，那么请在其提示时候输入密码。然后一路回车即可。 然后你会发现在你的家目录（~或者/home/username/）下多了一个叫做.ssh的文件夹，里面有两个文件id_rsa和id_rsa.pub。这两个就是SSH生成的文件，你需要将公钥传给远程服务器（Github相关问题详见Ubuntu16.04下Github配置博文），然后就可以免密码远程登录了。 多SSH配置建立不同的SSH配置文件首先你需要在家目录（~或者/home/username/）下建立一个.ssh文件夹，命令如下：1$ mkdir .ssh然后进入该文件夹（建议进入）：1$ cd ~/.ssh在该文件夹下，输入以下命令，后面我会介绍每个参数的作用：1$ ssh-keygen -t rsa -f ~/.ssh/id_rsa.blabla如果你需要将个人信息（如邮箱）加入进去（比如远程访问Github时），那么请执行如下命令：1$ ssh-keygen -t rsa -f ~/.ssh/id_rsa.blabla -C \"youraddress@youremail.com\"下面我们来分析下这两个命令。 -t rsa是指定你的加密算法。 -f是指定你的文件存储位置，我建议存在~/.ssh文件夹中。文件明明我建议按照我的格式写，blabla是该SSH配置文件的用途，比如.github, .localhost之类的。 -C注意这里C是大写字母。这里填写你的邮箱地址（顺便提一句配置Github时一定要填写你的注册邮箱，详见Ubuntu16.04下Github配置）。 建立索引因为你有多对SSH配置文件（.blabla和.blabla.pub是一对私钥和公钥），所以在远程登陆时，系统需要知道你需要将哪份私钥和远程的公钥进行匹配。所以你需要一个索引文件config。输入如下命令建立该文件：1$ touch config文件格式如下：1234Host name HostName hostname User username IdentityFile filepath一个config文件中可以有多个上述结构，每个结构之间建议用一个空行隔开。下面解析下这个结构。 Host就是个名字，每个结构之间不得重复 HostName是远程主机的域名，比如github.com, localhost之类或者是一个固定的IP地址。 User就是你登录该远程主机的用户名。 IdentityFile就是对应该主机的私钥的文件路径。依上述教程，应为~/.ssh/id_rsa.blabla。 登录localhost在配置软件环境时，有软件需要免密码登录localhost，也就是免密码登录本机。此时，你需要在.ssh目录下建立authorized_keys文件，命令如下：1$ touch authorized_keys建立此文件的目的是存储已知的SSH公钥信息。此时你需要将localhost的公钥复制进来。如果你是单SSH配置，则需要把id_rsa.pub文件复制进来。命令如下：1$ cat id_rsa.pub &gt;&gt; authorized_keys如果你是多SSH配置，依上述教程，你需要把id_rsa.localhost.pub复制进来，命令如下：1$ cat id_rsa.localhost.pub &gt;&gt; authorized_keys然后你就可以免密码登录localhost了。另外，如果需要添加其他已知SSH公钥的话，直接往authorized_keys中添加即可。 常见提示及应对方法第一次登录第一次远程登录一个新的主机时一般会出现如下提示：123The authenticity of host 'xxx.com (xx.xx.xx.xx)' can't be established.RSA key fingerprint is xx:xx...xx:xx.Are you sure you want to continue connecting (yes/no)?输入yes即可。然后可能会出现如下警告，意味着要永久存储该机器的特征信息，不用理睬即可：1Warning: Permanently added ‘xxx.com,xx.xx.xx.xx’ (RSA) to the list of known hosts. known_hosts文件当你访问远程主机时，系统会记录远程主机的特征信息，这些信息都存储在known_hosts里面。如果你不小心删掉了的话，也没什么事请，就是下一次进行SSH链接时还会出现第一次登录的提示，按照提示输入yes即可。","categories":[{"name":"Config","slug":"Config","permalink":"https://lrscy.github.io/categories/Config/"}],"tags":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://lrscy.github.io/tags/Ubuntu/"},{"name":"SSH","slug":"SSH","permalink":"https://lrscy.github.io/tags/SSH/"}]},{"title":"Ubuntu 16.04下Github配置","slug":"Ubuntu-Github-config","date":"2017-05-01T04:00:00.000Z","updated":"2020-07-09T08:11:49.693Z","comments":true,"path":"2017/05/01/Ubuntu-Github-config/","link":"","permalink":"https://lrscy.github.io/2017/05/01/Ubuntu-Github-config/","excerpt":"前言本文记录了我配置Github时候的全过程。 本文是在Ubuntu 16.04LTS下配置的，如果使用其他操作系统请自行查找对应命令或解决方案。 以下代码区域，带有$打头的表示需要在控制台（终端或称命令行）下面执行（不包括$符号）","text":"前言本文记录了我配置Github时候的全过程。 本文是在Ubuntu 16.04LTS下配置的，如果使用其他操作系统请自行查找对应命令或解决方案。 以下代码区域，带有$打头的表示需要在控制台（终端或称命令行）下面执行（不包括$符号） Git安装Ubuntu下需要安装git，输入命令如下：1$ sudo apt-get install git Github账户注册你需要一个Github的账户才能使用github.io服务。所以去github.com点击右上角的Sign Up注册即可。 SSH配置我们需要使用ssh来和github上的远程仓库进行通信，所以需要检查和配置ssh。 首先需要检查电脑上现有的ssh key:1$ cd ~/.ssh如果提示：No such file or dictionary 则说明你是第一次使用git。 如果你是第一次使用git，那么你需要做如下工作。否则可以跳过此节。 生成新的SSH KEY:1$ ssh-keygen -t rsa -C \"youraddress@youremail.com\"这里有几点需要注意的： -C中的C是大写字母 youraddress@youremail.com是你Github的注册邮箱其余的一路回车就能完成了。其中.pub是公钥，需要给到远程主机上，这点我们下一节再讲。 如果你需要连接多ssh终端，详见「Ubuntu16.04下(多)SSH配置」。 Github配置仅仅本地配置好了是不够的，你需要让你的github账户认识你。所以你需要按照以下步骤操作： 去登录github账户，在右上角点击头像找到Settings 点进去后点击左侧栏中的SSH and GPG keys 点击右侧New SSH key 随意输入个能带便当前机器的名字，并把本地.ssh目录下生成的关于github的.pub文件拷贝进来 保存即可 Github连接检查当一切配置妥当后，在终端中输入1$ ssh -T git@github.com如果你是第一次输入此命令，可能遇到如下提示：123The authenticity of host 'github.com (207.97.227.239)' can't be established.RSA key fingerprint is xx:xx...xx:xx.Are you sure you want to continue connecting (yes/no)?此时输入yes即可，然后会出现：1Hi xxx! You've successfully authenticated, but GitHub does not provide shell access.此时表明你的github配置完成了 本地个人信息配置此时你已经可以通过SSH连接到Github了，但是还有一些个人信息需要完善才能够和Github愉快的通信。 Git会依据本地设定的用户名和邮箱向远程主机提交更改，Github也是依据这些信息进行权限管理的。如果你当前只使用一个Github帐号，那么你需要以下两个命令来完善本地个人信息设定：12$ git config --global user.name \"your github name\"$ git config --global user.email \"youraddress@youremail.com\"上述命令中your github name是你的github用户名，youraddress@youremail.com是你github的注册邮箱。 如果你需要配置多Github账户，则需要用如下命令将–global所设置的参数撤销掉：12$ git config --global unset user.name$ git config --global unset user.email在每个git根目录下自行建立个人信息，命令如下：12$ git config user.name \"your github name\"$ git config user.email \"youraddress@youremail.com\"上述命令中your github name是你的github用户名，youraddress@youremail.com是你github的注册邮箱。 至此，Github配置已全部完成。","categories":[{"name":"Config","slug":"Config","permalink":"https://lrscy.github.io/categories/Config/"}],"tags":[{"name":"Github","slug":"Github","permalink":"https://lrscy.github.io/tags/Github/"},{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://lrscy.github.io/tags/Ubuntu/"},{"name":"SSH","slug":"SSH","permalink":"https://lrscy.github.io/tags/SSH/"}]},{"title":"Ubuntu 16.04下Ruby基础配置","slug":"Ubuntu-Ruby-base-config","date":"2017-05-01T04:00:00.000Z","updated":"2020-07-09T08:11:49.693Z","comments":true,"path":"2017/05/01/Ubuntu-Ruby-base-config/","link":"","permalink":"https://lrscy.github.io/2017/05/01/Ubuntu-Ruby-base-config/","excerpt":"Note: 本文内容节选复述了Ruby-China中的教程中的内容，其中有一两步和原版有出入（我踩到的坑）。如果需要看原版内容请点击上述链接。 本文是在Ubuntu 16.04 LTS环境下配置的，如果使用其他操作系统请自行查找对应命令或解决方案。","text":"Note: 本文内容节选复述了Ruby-China中的教程中的内容，其中有一两步和原版有出入（我踩到的坑）。如果需要看原版内容请点击上述链接。 本文是在Ubuntu 16.04 LTS环境下配置的，如果使用其他操作系统请自行查找对应命令或解决方案。 以下代码区域，带有$打头的表示需要在控制台（终端或称命令行）下面执行（不包括$符号） 由于ubuntu自带的ruby版本太老，所以需要从下列途径更新。 安装系统需要的包12$ sudo apt-get install curl$ ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\" 其中curl是一种下载工具，在后续操作中需要用到。 安装RVMRVM安装只需输入以下命令：1234$ gpg --keyserver hkp://keys.gnupg.net --recv-keys 409B6B1796C275462A1703113804BB82D39DC0E3$ curl -sSL https://get.rvm.io | bash -s stable# 如果上面的连接失败，可以尝试: $ curl -L https://raw.githubusercontent.com/wayneeseguin/rvm/master/binscripts/rvm-installer | bash -s stable如果其中某些命令出现权限不足提醒，则在前面添加sudo再执行即可。 接下来载入RVM环境1$ source ~/.rvm/scripts/rvm接下来有一点和教程中不太一样（我踩到的坑）。打开~/.bash_profile文件你可能发现如下一行：1[[ -s \"$HOME/.rvm/scripts/rvm\" ]] &amp;&amp; source \"$HOME/.rvm/scripts/rvm\" # Load RVM into a shell session *as a function*原文中说“新开的终端就不想要这么做了，会自动重新载入的”正因为RVM安装程序在.bash_profile中添加了这么一行。然而你可能发现现实很残酷，新开的终端并没有载入rvm环境。此时你需要在家目录（～）下的.bashrc文件中的末尾添加如下几行：12# Add RVM to PATH for scripting. Make sure this is the last PATH variable change.[[ -s \"$HOME/.rvm/scripts/rvm\" ]] &amp;&amp; source \"$HOME/.rvm/scripts/rvm\"其实也就是复制到.bashrc中而已。然后重启终端即可，如果还不行那么注销重新登录即可，如果还不行请重启机器。 然后修改RVM下的Ruby源，到Ruby China的镜像：1$ echo \"ruby_url=https://cache.ruby-china.org/pub/ruby\" &gt; ~/.rvm/user/db检查下是否安装正确1$ rvm -v 用RVM按转Ruby环境12$ rvm requirements$ rvm instll 2.4.0 这里的版本号（2.4.0）可以通过下属命令查看并可修改：1$ rvm list 设置Ruby版本RVM 安装好后，需要执行下述命令将指定版本设为系统默认版本1$ rvm use 2.4.0 --default这个时候你可以测试是否正确12$ ruby -v$ gem -v这里你可以替换原有的gem源到Ruby China的源或者淘宝源，分别是：12$ gem sources --add https://gems.ruby-china.org/ --remove https://rubygems.org/$ gem sources --add https://ruby.taobao.org/ --remove https://rubygems.org/可以通过下属命令查看gem源：1$ gem sources -l接下来安装Bundler1$ gem install bundler 设置Rails环境输入以下命令就可以轻松安装上Rails了：1$ gem install rails测试是否安装正确1$ rails -v至此，Ruby基础安装教程到此结束喵～","categories":[{"name":"Config","slug":"Config","permalink":"https://lrscy.github.io/categories/Config/"}],"tags":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://lrscy.github.io/tags/Ubuntu/"},{"name":"Ruby","slug":"Ruby","permalink":"https://lrscy.github.io/tags/Ruby/"}]},{"title":"Ubuntu 16.04下从零起步搭建配置github.io博客——Jekyll","slug":"Ubuntu-Github-io-config-Jekyll","date":"2017-04-30T04:00:00.000Z","updated":"2020-07-09T08:11:49.693Z","comments":true,"path":"2017/04/30/Ubuntu-Github-io-config-Jekyll/","link":"","permalink":"https://lrscy.github.io/2017/04/30/Ubuntu-Github-io-config-Jekyll/","excerpt":"前言本文旨在将我配置github.io博客全过程展现出来，帮助从零起步的小白们一步一步的配置属于自己的github.io博客。如果过程正哪一步骤错了，请各位大佬指出，谢谢～ 小白的入门门槛： 需要耐心，耐心，耐心碰到问题自主去学习和在网上寻找答案 本文是在Ubuntu 16.04 LTS环境下配置的，如果使用其他操作系统请自行查找对应命令或解决方案。 以下代码区域，带有$打头的表示需要在控制台（终端或称命令行）下面执行（不包括$符号） 本文几乎所有命令都可以直接拷到控制台（终端或称命令行）内直接执行而不用理解其具体含义（除非特殊表明需要修改），但是强烈不建议这么做！！！ 我是基于Jekyll模板搭建的博客，所以不会讲如何从零手敲出一个博客样式出来，但会比较详细的讲模板中哪里需要修改。","text":"前言本文旨在将我配置github.io博客全过程展现出来，帮助从零起步的小白们一步一步的配置属于自己的github.io博客。如果过程正哪一步骤错了，请各位大佬指出，谢谢～ 小白的入门门槛： 需要耐心，耐心，耐心碰到问题自主去学习和在网上寻找答案 本文是在Ubuntu 16.04 LTS环境下配置的，如果使用其他操作系统请自行查找对应命令或解决方案。 以下代码区域，带有$打头的表示需要在控制台（终端或称命令行）下面执行（不包括$符号） 本文几乎所有命令都可以直接拷到控制台（终端或称命令行）内直接执行而不用理解其具体含义（除非特殊表明需要修改），但是强烈不建议这么做！！！ 我是基于Jekyll模板搭建的博客，所以不会讲如何从零手敲出一个博客样式出来，但会比较详细的讲模板中哪里需要修改。 基础环境搭建Git/Github/SSH配置详见我的博客「Ubuntu16.04下Gtihub配置」。 本地Jekyll环境配置因为github.io博客是基于Jekyll模板生成的，所以需要了解下Jekyll模板。其实本地不配置Jekyll也是可以的，不过后果就是无法进行本地预览。提交到Github上平均需要10分钟才能看到更改结果，所以我是建议本地配置下Jekyll的。 Ruby安装配置Jekyll是基于ruby的，所以要安装Jekyll还需要安装ruby, gem等。不过不要担心，我已经把雷都踩过了，你只需要跟着我一步步走就好。不过，我们不需要把Rails环境配上，因为我们暂时用不到。 详见我的博客「Ubuntu16.04下Ruby基础配置」。 Jekyll安装配置由于上一步已经安装好了Ruby和gem了，所以只需要下面一条命令就可以安上Jekyll：1$ gem install jekyll没了，嗯～ 真的没骗你～喵～ 至此基础环境搭建也就完成了。 Jekyll目录介绍我会对Jekyll目录进行一个简单粗略的介绍，让你知道每个目录大概都是做什么的，便于你以后查找需要修改的文件的位置。 Note: 我的博客是基于其他作者些的Jekyll主题改的，所以以下部分所述“不需要修改”皆出于此出发点。对于有前端基础的同学请自行忽略。 下面是目录树：123456789101112131415161718192021blog _includes footer.html head.html header.html _layouts default.html page.html post.html _posts 2017-04-30-Hello.md _sass _site css main.scss .gitignore .sass-cache _config.yml about.md feed.xml index.html _include这里都是网页模块文件，用来加载到你的布局或文章中。以后可能需要修改部分内容。可以在其他文件中采用如下方法调用该文件夹内文件：1&#123; % include file %&#125;Note: 调用时候把{和%之间的空格去掉 _layouts存放网页模板。每个网页只需要关注自己的内容。也基本上不用修改。 _posts这里就是存放我们博文的地方了。文件名称非常非常关键，必须使用统一的格式：YEAR-MONTH-DAY-TITLE.md且中间不能有空格。例如：2017-04-30-Hello.md。不是此格式的博文不会被解析也不会在网站中显示。 _sass存放网站用到的sass文件。基本上你不用管这里面做了些什么。 _site在Jekyll解析完成这些文件后，会将最终的静态网站源代码默认的放在这个文件夹下来保存。这个文件夹最好添加进.gitignore文件中（这个问题我们后面再说）。 css存放网页样式文件。基本上也不用管。 .sass-cachesass的编译缓存文件。基本上你不用管这里面都是什么。 _config.yml最重要的配置文件，这里面决定了Jekyll如何解析网站源代码。官方有给出配置文件详细信息，详情请见这里。 _plugins你可能需要这个文件夹也可能不需要，按需建立此文件夹。这里用来存放Jekyll插件。 正式开始搭建博客终于开始搭博客了喵～ Github仓库建立本节是在复述官方教程中的内容。 建立一个仓库你需要在你的Github中建立一个新的仓库，名字必须是{your github username}.github.io，否则Github不会将其认为是Github博客。 克隆到本地采用下述命令进行复制1$ git clone https://github.com/username/username.github.io这里的username是你的github用户名 至此，Github部分完结 Jekyll模板我的博客是基于Pithy主题建立的。Jekyll模板可以从Jekyll Theme上或大佬们的Github上下载。下载解压后直接将所有文件都拷贝到自己刚克隆下来的文件夹下即可。 _config.yml解压后唯一需要修改的部分，修改其中所有你认识的英文，如果没有可以填的空着就好。 接下来你需要做的就是将其push到远程仓库里。如果你不熟悉Git操作命令，我强烈建议你去看下廖雪峰的Git教程。 添加.gitignore文件上文介绍jekyll结构的时候说了句_site要加入.gitignore文件中。.gitignore文件是git上传时要过滤掉的文件。依据自己需求改下就可以了。我目前的.gitignore文件如下：1234_site/*.sass-cache.jekyll-metadata.swp搭完了，嗯，你没看错，搭完了喵～ 添加Mathjax支持作为一个技术小白，就算再小白也有需要用到数学公式的时候。这时mathjax能满足你的大部分需求。Mathjax配置很简单，过程如下： 在_include/head.html中添加以下代码：1234567891011&lt;script type=\"text/x-mathjax-config\"&gt; MathJax.Hub.Config(&#123; TeX: &#123; equationNumbers: &#123; autoNumber: \"AMS\" &#125; &#125;, tex2jax: &#123; inlineMath: [ ['$','$'], ['\\\\(', '\\\\)'] ], displayMath: [ ['$$','$$'] ], processEscapes: true, &#125; &#125;);&lt;/script&gt;&lt;script type=\"text/javascript\" async src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML\"&gt;&lt;/script&gt;最后一行那里的src，有些博客包括官方教程些的都是http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML，然而在https下此网址被认为是不安全的而一些浏览器被屏蔽了（比如Chrome），所以需要换成这个网址。 然后你需要检查下_config.yml中是否有如下一行：1markdown: kramdown如果没有添加上即可。这是在指定markdown的解释器，如果你想换成其他的也可以。 至此，Mathjax配置完成了。用在行内用$来包裹latex公式，行间公式需要用$$包裹住。例子如下：123456# 第一种blabla$x$blabla# 第二种$$P(y|x)$$ 添加代码高亮我选择的是Jekyll原生支持的rouge进行代码高亮。只需要在_config.yml中添加一行：1highlighter: rouge即可高亮代码。 最后的检测现在你已经配好了所有功能，在git仓库的根目录下运行jekyll serve即可以迅速在本地生成博客。通过浏览器访问localhost:4000即可看到成果啦喵～ 最后的最后记得push到你Github的远程仓库中，然后就可以在网页上看到你自己的博客了。","categories":[{"name":"Config","slug":"Config","permalink":"https://lrscy.github.io/categories/Config/"}],"tags":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://lrscy.github.io/tags/Ubuntu/"},{"name":"Github.io","slug":"Github-io","permalink":"https://lrscy.github.io/tags/Github-io/"},{"name":"Jekyll","slug":"Jekyll","permalink":"https://lrscy.github.io/tags/Jekyll/"}]},{"title":"「译」GPU神经网络机器翻译导论（第一部分）","slug":"Trans-Intro-to-NMT-with-GPUs-part1","date":"2017-04-29T04:00:00.000Z","updated":"2020-07-09T08:11:49.693Z","comments":true,"path":"2017/04/29/Trans-Intro-to-NMT-with-GPUs-part1/","link":"","permalink":"https://lrscy.github.io/2017/04/29/Trans-Intro-to-NMT-with-GPUs-part1/","excerpt":"原文名：Introduction to Neural Machine Translation with GPUs (part 1) 翻译自NVIDIA 深度学习系列 「译者注：博文中有些链接指向Google Drive，需要各位同学科学上网查阅资料。」 注意：这篇是Kyunghyun Cho写的神经网络机器翻译系列中的第一篇。其余的请见第二部分和第三部分。","text":"原文名：Introduction to Neural Machine Translation with GPUs (part 1) 翻译自NVIDIA 深度学习系列 「译者注：博文中有些链接指向Google Drive，需要各位同学科学上网查阅资料。」 注意：这篇是Kyunghyun Cho写的神经网络机器翻译系列中的第一篇。其余的请见第二部分和第三部分。 神经网络机器翻译是近期被提出的的一个框架，其只基于神经网络。此篇文章是该系列的第一篇文章，我将阐述一个简单的编码-解码模型来构建一个神经网络机器翻译系统「Cho et al., 2014; Sutskever et al., 2014; Kalchbrenner and Blunsom, 2013」。后面的文章我将阐述如何将注意力机制融入简单的编码-解码模型中「Bahdanau et al., 2015」，并且形成好的英法、英德、英土和英中翻译模型「Gulcehre et al., 2015; Jean et al., 2015」。此外，我将介绍一些近期最新的进展——将此神经网络机器翻译框架应用于描述图像和视频的。 统计机器翻译首先，我们简单的阐述下机器翻译。事实上，机器翻译是个统称。我们希望机器将一种语言（我们称之为源语言）翻译成另一种语言（我们称之为目标语言）。（尽管理想情况下，机器应该能够将一整篇文档从一种语言翻译到另一种语言，但是我们在此篇博客中专注于句子级别的机器翻译。） 我们有许多方式去构建一台能够翻译语言的机器。例如，我们可以请教一个同时会两门语言的的人为我们构建一套翻译规则来将源语言正确的翻译成目标语言。但是这不是一个好的解决方案，联想下我们自己，我们可能都无法完整的构建出我们母语的语法规则，更别提构建两个语言规则之间的对应了。写一套详尽的翻译规则将源语言翻译成目标语言更是痴心妄想。因此，在此篇博文中，我们将专注于利用统计的方法，通过大量的文本语料库来提取这些隐式和显式的规则。 这种通过统计方法进行机器翻译的方法被成为统计机器翻译。虽然最终目标是相同的（建立一个能将一个句子从一种语言翻译到另一种语言的机器），但是我们让机器从数据中学习如何翻译而非学习如何构建规则（如图1的图解）。此学习是基于统计的方法，每个学习过机器学习的人都应该很熟悉此方法。事实上，统计机器翻译只是机器学习的一种特定的应用——寻找一种从源语言到相关目标语言的对应关系。 图1. 统计机器翻译 机器翻译的一个重要的特征是翻译函数并非像其他机器学习应用一样是一对一的或多对一的（例如分类应用的函数就是多对一的），而是一对多的，也就是说一个源句子可能有多个可能的翻译结果。正因如此，模型的方法不是个确定性函数而是一个目标句子$y$对$x$的条件概率$p(y|x)$。条件概率能够对于多个分离度良好的结构或句子分配同等的高概率，这就使得源语言和目标语言的一对多关系得以成立。 现在，我们假设你将建立一个统计机器翻译系统并且要将英语翻译成法语。第一步且可能是最重要的一步是收集平行语料。我们用$x^n$和$y^n$来分别表示源语句和对应的翻译语句。上标$n$代表这是第$n$对语句（通常我们需要成千上万对语句才能训练出一个好的翻译模型）。我将用$D=(x^1,y^1),…,(x^N,y^N)$来表示数据中有$N$对语句对。 我们从哪里能得到这些训练语句对呢？对于机器翻译中广泛使用的语言，你可能想要查看统计机器翻译研讨会 (EMNLP)或者国际语言翻译研讨会 (IWSLT)。 在有了训练集$D=(x^1,y^1),…,(x^N,y^N)$后，我们能够通过观察这个模型在训练集$D$上的表现给这个模型打分。这分数（我将称之为模型的对数似然）是该模型对每一对$(x^n,y^n)$的对数似然的平均值。利用机器翻译模型的概率解释，模型在每一对上的对数似然只是模型给这一对语料$log⁡(y^n|x^n,θ)$评估的对数概率有多高。然后，整个模型的在训练集上的打分如下： $$ \\mathcal{L}(\\theta, D) = \\sum_{(x^n, y^n) \\in D} \\log p(y^n|x^n, \\theta) $$ 如果对数似然值$\\mathcal{L}$很低，说明该模型没有分配给正确翻译句对足够的概率，也意味着该模型将部分概率分配给了错误的翻译。所以，我们希望找到一种模型的结构或者参数$θ$来最大化对数似然或得分。 在机器学习中，这被称为最大似然估计。除此之外我们还有一个更重要的问题：我们如何建立模型$p(y|x,θ)$？ 统计机器翻译简述IBM T.J. 沃森研究中心「Brown et al., 1993等其他参考文献」在20多年前就已经提出并回答了如何建立条件分布模型。从那时起，统计机器翻译(SMT)的核心就是一个线性对数模型，我们用有许多特征的线性组合近似了$p(y|x)$的对数： $$ \\log p(y|x) \\approx \\log p(y|x, \\theta) = \\sum_i \\theta_i f_i(x, y) + C(\\theta), $$ 其中$C$是归一化常数。在这种情况下，很大一部分研究工作是要找到一组很好的特征函数f_i，并且有一本书已经详尽描述了关于其的所有细节「Koehn」。 在这种统计机器翻译方法中，通常来说机器学习需要做的只是寻找到一组能够平衡不同特征的系数θ_i，或者从对数线性模型「Schwenk, 2007」中筛选或重新排序一组可能的翻译。更具体来所，神经网络已经被用作组成特征函数功能的一部分，也可以用来重新排序所谓的最佳可能翻译的列表，就像图2中中间和右边部分。 图2. 图解NMT，SMT+神经网络重新排序和SMT-NN。从「Bahadanau et al., 2015」在ICLR2015中的幻灯片中截取 另一方面，在此篇博客中，我将专注于最近提出一种称为神经网络机器翻译的方法。其中机器学习，特别是神经网络，拥有更多甚至全部的控制权。正如图2中左边部分所展示的。 神经网络机器翻译就像普通的深度学习一样，神经网络机器翻译(NMT)不依赖于已经提前设计好的方法。（通过提前设计好的方法，也就是说那些没有学习的功能。）相比而言，NMT设计的目标是设计一个完全可训练的模型，其每一部分都是基于语料库进行调整，以最优化其翻译表现。 一个完全可训练的NMT模型$\\mathcal{M}$从尽可能的地道的表述源语言语句开始训练，到生成尽可能地道的目标语言语句停止。目前，我们来考虑一个词序列视作作一个句子的最原始的表示。（虽然对于大多数语言来说这并不合理，但是在尽可能的保证一般性的前提下，我会将词作为一个语言的最小的单位。）每一个序列中的词都被它在字典中的索引数字代替。例如，在基于词频率的英语词典中，第一个出现的词会被表示为整数1。我将用$X=(x_1,x_2, \\cdots, x_T)$来表示源句子，用$Y=(y_1, y_2, \\cdots, y_{T’})$来表示目标句子。 给出源句子$X=(x_1,x_2, \\cdots, x_T)$的词索引，NMT模型$\\mathcal{M}$会计算$Y=(y_1, y_2, \\cdots, y_{T’})$的条件概率。接下来，我将阐述我们如何来建立一个神经网络来近似条件概率的条件概率。接下来，我将阐述我们如何来建立一个神经网络来近似条件概率$p(Y|X)$。 循环神经网络机器翻译的一个重要的特征，或是说基于自然语言的任何任务，是处理可变长度输入$X=(x_1,x_2, \\cdots, x_T)$和可变长度输出$Y=(y_1, y_2, \\cdots, y_{T’})$。换句话说，$T$和$T’$不固定。 为了处理这些可变长度的输入和输出，我们需要用到循环神经网络（RNN）。目前广泛应用的前馈神经网络（比如卷积神经网络）除了网络自身的参数外不保留中间状态。无论何时，一个样例进入前馈神经网络，无论网络内部参数还是隐藏层的激活都是重新计算的而不受前一个样本的状态结果的影响。然而RNN在读入一个序列时保存了其内部状态（在当前情况下是词序列），因此能够处理任何长度的输入。 我接下来将更详细的解释一下RNN。RNN的主要的思想是通过使用递归将输入的序列压缩成一个固定维度的向量。假设在第$t$步我们有一个向量$h_{t-1}$保存了之前所有的符号的状态。RNN将计算出一个新的向量（或称为内部状态），$h_t$通过下式压缩了之前所有符号$\\left(x_1, x_2, \\dots, x_{t-1} \\right)$包括新的符号$x_t$： $$ h_t = \\phi_{\\theta}(x_t, h_{t-1}) $$ 其中$\\phi_{\\theta}$是由$\\theta$参数化的一个函数，以新符号$x_t$和保存了前$(t−1)$个符号的历史状态$h_{t-1}$作为输入。最开始，我们可以放心的假设$h_0$是一个全零向量。 图3. 图解不同类型的循环神经网络。摘自「Pascanu et al., 2014」 递归激活函数$\\phi$通常被实现为一个非线性函数套着一个放射变换： $$ h_t=\\tanh(Wx_t + Uh_{t-1} + b) $$ 在这个等式中，参数包括输入权重矩阵$W$，循环权重矩阵$U$和一个偏差向量$b$。我必须强调这不是唯一的实现方案，现在依旧有很多的机会来设计新的循环激活函数。见图三的一些例子「Pascanu et al., 2014」。 这种简单的RNN可以轻易的由Theano来实现，且Theano可以让你的RNN程序在CPU和GPU下透明的运行。详见循环神经网络实现词向量；注意，整个RNN代码总共不超过10行！ 最近，研究发现用更复杂的激活函数来训练循环神经网络有更好的效果且更加容易，比如LSTM「Hochreiter and Schmidhuber, 1997」和GRU「Cho et al., 2014」。 $$\\begin{align}r_t &amp;= \\sigma(W_rx_t + Urh_{t-1} + b_r) \\\\u_t &amp;= \\sigma(W_ux_t + r_t \\odot (U_uh_{t-1}) + b_u) \\\\h_t &amp;= u_t \\odot h_{t-1} + (1-u_t) \\odot \\tanh(Wx_t + r_t \\odot (U_uh_{t-1}) + b)\\end{align}$$ 正如上述情况下的简单循环激活函数，参数包括了输入矩阵$W$($W_r$和$W_u$)，循环神经矩阵$U$($U_r$和$U_u$)和偏差向量$b$($b_r$和$b_u$)。 虽然这些单元看起来比简单RNN复杂些，但是由Theano或者其他深度学习框架（比如Torch）实现起来会很简单。例如LSTM网络进行情感分析(样例代码)。 我已经将RNN表述为了一个历史压缩器，但是它也可以用来为一个序列进行概率建模。给一个序列进行概率建模的意思是让机器学习一个模型来计算任意给定序列$X=(x_1, x_2, \\cdots, x_T)$的概率$p(X)$。我们如何设计$p(X)$才能让其满足递归的形式呢？ 我们从重新将$p(X)=p(x_1, x_2, \\cdots, x_{T})$描述为下式开始： $$ p(x_1, x_2, \\cdots, x{T})=p(x_1)p(x_2|x_1)p(x_3|x_1, x_2) \\cdots p(x_T|x1, \\cdots, x_{T-1}) $$ 上述改变基于条件概率公式，$p(X|Y)=\\frac{P(X,Y)}{P(Y)}$。从上述改变我们可以看出，我们可以设计出一个递归表达式，例如： $$ p(x_1)p(x_2|x_1)p(x_3|x_1, x_2) \\cdots p(x_T|x_1, \\cdots, x_{T-1}) = \\prod_{t=1}^{T}p(x_t|x_{\\&lt;t}) $$ 现在我们使一个RNN模型$p(x_t|x_{\\&lt;t})$在每一步t$时进行如下操作： $$p(x_t|x_{\\&lt;t}) = g_{\\theta}(h_{t-1}) \\\\h_{t-1} = \\phi_{\\theta}(x_{t-1}, h_{t-2})$$ $g_{\\theta}$通过$h_{t-1}$输出基于前$(t-1)$个符号全部历史状态的条件分布概率。换句话说，在每个时刻，RNN试图通过学习输入符号的历史数据预测下一个字符应该是什么。 RNN有许多有趣的属性和特点值得我用上好几个小时来讲述，但是由于这是个博客，我不得就此停止。自此以后，我将讲述你们开始建立神经网络系统前的所有必备的知识。如果你想对RNN有更多的了解的话，我建议你去阅读下述论文。但是很明显，这些论文也并不能穷尽所有RNN有关的知识。你也可以去看我的关于如何将RNN应用于语言模型的幻灯片。 Graves, Alex. “Generating sequences with recurrent neural networks.” arXiv preprint arXiv:1308.0850 (2013). Pascanu, Razvan et al. “How to construct deep recurrent neural networks.” arXiv preprint arXiv:1312.6026 (2013). Boulanger-Lewandowski, Nicolas, Yoshua Bengio, and Pascal Vincent. “Modeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation and transcription.” arXiv preprint arXiv:1206.6392 (2012). Mikolov, Tomas et al. “Recurrent neural network based language model.” INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association, Makuhari, Chiba, Japan, September 26-30, 2010 1 Jan. 2010: 1045-1048. Hochreiter, Sepp, and Jürgen Schmidhuber. “Long short-term memory.” Neural computation 9.8 (1997): 1735-1780. Cho, Kyunghyun et al. “Learning phrase representations using rnn encoder-decoder for statistical machine translation.” arXiv preprint arXiv:1406.1078 (2014). Bengio, Yoshua, Patrice Simard, and Paolo Frasconi. “Learning long-term dependencies with gradient descent is difficult.” Neural Networks, IEEE Transactions on 5.2 (1994): 157-166. 接下来的要讲述的事情在这篇博文中，我介绍了机器翻译，描述了统计机器翻译如何解决机器翻译的问题。在讲述统计机器翻译框架时，我讨论了神经网络如何用来提高翻译的整体表现。 这系列博客的目标是介绍一个新的神经网络机器翻译模型；这篇博文奠定了基础，重点介绍了循环神经网络的两个核心能力：序列总结能力和序列概率建模能力。 基于这两个特性，在下一篇博客中，我将讲述一个完整的基于循环神经网络的神经网络机器翻译系统。我也会想你展示为什么GPU对于神经网络机器翻译这么重要！","categories":[{"name":"NLP","slug":"NLP","permalink":"https://lrscy.github.io/categories/NLP/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"https://lrscy.github.io/tags/NLP/"},{"name":"NMT","slug":"NMT","permalink":"https://lrscy.github.io/tags/NMT/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"https://lrscy.github.io/tags/Deep-Learning/"},{"name":"GPU","slug":"GPU","permalink":"https://lrscy.github.io/tags/GPU/"},{"name":"Theano","slug":"Theano","permalink":"https://lrscy.github.io/tags/Theano/"}]}]}